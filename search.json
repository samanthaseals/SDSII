[
  {
    "objectID": "files/schedule.html",
    "href": "files/schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "All .qmd files for lectures can be found on GitHub: lecture folder\nAll data used in lecture can also be found on GitHub: data folder\n\n\n\n\n\n\nWeek 1: Review of Linear Regression\n\n\n\n\n\n\nLecture Schedule:\n\nReview of Linear Regression slides qmd\nVisualizing the Model slides qmd \nModel Assumptions slides qmd\nModel Diagnostics slides qmd\n\nCheck in 1 (due Sunday evening)\nQuiz 1 (due Tuesday evening)\nProject 1 (due Tuesday evening)\n\n\n\n\n\n\n\n\n\n\nWeek 2: More on Linear Regression\n\n\n\n\n\n\nLecture Schedule:\n\nCategorical Predictors: Data Management slides qmd\nCategorical Predictors: Modeling & Interpretation slides qmd\nCategorical Predictors: Statistical Inference slides qmd \nVisualizing the Model with Categorical Predictors slides qmd\n\nCheck in 2 (due Sunday evening)\nQuiz 2 (due Tuesday evening)\nProject 2 (due Tuesday evening)\n\n\n\n\n\n\n\n\n\n\nWeek 3: More on Linear Regression\n\n\n\n\n\n\nLecture Schedule:\n\nInteractions: Continuous \\(\\times\\) Continuous\nInteractions: Including Categorical Terms\nSimplfying Models\nVisualizing the Model with Interactions\n\nCheck in 3 (due Sunday evening)\nQuiz 3 (due Tuesday evening)\nProject 3 (due Tuesday evening)\n\n\n\n\n\n\n\n\n\n\nWeek 4: Gamma and Beta Regressions\n\n\n\n\n\n\nLecture Schedule:\n\nGamma Regression\nVisualizing the Model: Gamma\nBeta Regression\nVisualizing the Model: Beta\n\nCheck in 4 (due Sunday evening)\nQuiz 4 (due Tuesday evening)\nProject 4 (due Tuesday evening)\n\n\n\n\n\n\n\n\n\n\nWeek 5: Binary, Ordinal, and Nominal Logistic Regressions\n\n\n\n\n\n\nLecture Schedule:\n\nBinary Logistic Regression\nOrdinal Logistic Regression\nNominal Logistic Regression\nVisualizing Logistic Regression Models\n\nCheck in 5 (due Sunday evening)\nQuiz 5 (due Tuesday evening)\nProject 5 (due Tuesday evening)\n\n\n\n\n\n\n\n\n\n\nWeek 6: Poisson and Negative Binomial Regressions\n\n\n\n\n\n\nLecture Schedule:\n\nPoisson Regression\nNegative Binomial Regression\nZero-Inflated Models\nVisualizing the Model: Count Data\n\nCheck in 6 (due Sunday evening)\nQuiz 6 (due Tuesday evening)\nProject 6 (due Tuesday evening)\n\n\n\n\n\n\n\n\n\n\nWeek 7: Choosing a Distribution\n\n\n\n\n\n\nLecture Schedule:\n\nChoosing the Right Modeling Approach\n\nCheck in 7 (due Sunday evening)\nQuiz 7 (due Tuesday evening)\nProject 7 (due Tuesday evening)\n\n\n\n\n\n\n\n\n\n\nWeek 8: Final Exam\n\n\n\n\n\n\nFinal exam opens on Friday morning at 12 am and is due by Friday evening at 11:59 pm, Central.\n\nThis is a timed exam; once you open the exam on Canvas, you will have 3 hours for completion.\nThe exam is cumulative and will cover all material from the course.\nThe exam is conceptual and applied; none of the questions will require/ask for coding.\nNo make up exams will be given."
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#introduction",
    "href": "files/lectures/W1_L2_visualization.html#introduction",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\n\nRecall the general linear model, y = \\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k\n\n\\beta_0 is the y-intercept, or the average outcome (y) when all x_i = 0.\n\\beta_i is the slope for predictor i and describes the relationship between the predictor and the outcome, after adjusting (or accounting) for the other predictors in the model.\n\nIn the last lecture, we used a linear model to explore the relationships between clubhouse happiness and laughs, grumbles, and time spent with friends."
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#lecture-example-set-up",
    "href": "files/lectures/W1_L2_visualization.html#lecture-example-set-up",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Lecture Example Set Up",
    "text": "Lecture Example Set Up\n\n\nOn a busy day at the clubhouse, Mickey Mouse wants to understand what drives “happiness” at the end of the day. For each day, he records (in the clubhouse dataset):\n\nTime with friends (in hours; time_with_friends): how many hours Mickey spends hanging out with his friends.\nGoofy Laughs (a count; goofy_laughs) – how many big goofy laughs happen that day.\nDonald Grumbles (a count; donald_grumbles): how many times Donald gets frustrated and grumbles.\nClubhouse Happiness (a score; clubhouse_happiness): an overall happiness score at the end of the day.\n\n\n\nclubhouse &lt;- read_csv(\"https://raw.githubusercontent.com/samanthaseals/SDSII/refs/heads/main/files/data/lectures/W1_mickey_clubhouse.csv\")"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#example-1-simple-linear-regression",
    "href": "files/lectures/W1_L2_visualization.html#example-1-simple-linear-regression",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Example 1: Simple Linear Regression",
    "text": "Example 1: Simple Linear Regression\n\nFor our first example, let’s look at clubhouse happiness as a function of time spent with friends.\n\n\nm1 &lt;- glm(clubhouse_happiness ~ time_with_friends, \n         family = \"gaussian\",\n         data = clubhouse)\nm1 %&gt;% tidy()\n\n\n  \n\n\n\n\n\\hat{\\text{happiness}} = 57.55 + 3.49 \\text{ time}"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#model-visualization-simple-linear-regression",
    "href": "files/lectures/W1_L2_visualization.html#model-visualization-simple-linear-regression",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Model Visualization: Simple Linear Regression",
    "text": "Model Visualization: Simple Linear Regression\n\nWe can visualize this simple linear regression model with a scatterplot and regression line.\nTo create the regression line, we need to create predicted values from our model.\n\n\nclubhouse &lt;- clubhouse %&gt;%\n  mutate(predicted_happiness_k1 = 57.55 + 3.49*time_with_friends)\n\n\n\n\n\n\n  \n\n\n\n\nNow we can plot the data and the regression line."
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)\n\nThe ggplot() function initializes a ggplot object.\n\n\ndataset_name %&gt;% ggplot()\n\n\nIn our example,\n\n\nclubhouse %&gt;% ggplot()"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-1",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-1",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-2",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-2",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)\n\nWe must define the aesthetics (i.e., the x and y variables) inside ggplot().\n\n\ndataset_name %&gt;% ggplot(aes(x = variable_on_x,\n                            y = variable_on_y))\n\n\nIn our example,\n\n\nclubhouse %&gt;% ggplot(aes(x = time_with_friends,\n                         y = clubhouse_happiness))"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-3",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-3",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-4",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-4",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)\n\nWe must add geom_TYPE() layers to actually see anything on the plot.\n\nWe layer multiple geoms to the plot using + operator.\n\n\n\ndataset_name %&gt;% ggplot(aes(x = variable_on_x,\n                            y = variable_on_y)) +\n  geom_TYPE()\n\n\nIn our example,\n\n\nclubhouse %&gt;% ggplot(aes(x = time_with_friends,\n                         y = clubhouse_happiness)) +\n  geom_point()"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-5",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-5",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-6",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-6",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)\n\nWe continue to layer our plot with additional geom_TYPE()s,\n\n\ndataset_name %&gt;% ggplot(aes(x = variable_on_x,\n                            y = variable_on_y)) +\n  geom_TYPE() +\n  geom_TYPE()\n\n\nIn our example,\n\n\nclubhouse %&gt;% ggplot(aes(x = time_with_friends,\n                         y = clubhouse_happiness)) +\n  geom_point() + \n  geom_line()"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-7",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-7",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-8",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-8",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)\n\nOoops! That geom_line() didn’t work as expected.\n\nIn the ggplot(), we set y to be the actual happiness values (clubhouse_happiness).\nWe now need to overwrite the y variable to be the the predicted values from our model (predicted_happiness).\n\n\n\ndataset_name %&gt;% ggplot(aes(x = variable_on_x,\n                            y = variable_on_y)) +\n  geom_TYPE() +\n  geom_TYPE(aes(y = predicted_y))\n\n\nIn our example,\n\n\nclubhouse %&gt;% ggplot(aes(x = time_with_friends,\n                         y = clubhouse_happiness)) +\n  geom_point() + \n  geom_line(aes(y = predicted_happiness_k1))"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-9",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-9",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-10",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-10",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)\n\nNow, we can work on “prettying” up our plot.\n\nMy first step is to change the theme using theme_NAME().\n\n\n\ndataset_name %&gt;% ggplot(aes(x = variable_on_x,\n                            y = variable_on_y)) +\n  geom_TYPE() +\n  geom_TYPE(aes(y = predicted_y)) +\n  theme_NAME()\n\n\nIn our example,\n\n\nclubhouse %&gt;% ggplot(aes(x = time_with_friends,\n                         y = clubhouse_happiness)) +\n  geom_point() + \n  geom_line(aes(y = predicted_happiness_k1)) +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-11",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-11",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-12",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-12",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)\n\nNow, we can work on “prettying” up our plot.\n\nThen, I want to clean up the axis titles.\n\n\n\ndataset_name %&gt;% ggplot(aes(x = variable_on_x,\n                            y = variable_on_y)) +\n  geom_TYPE() +\n  geom_TYPE(aes(y = predicted_y)) +\n  labs(x = \"x axis title\",\n       y = \"y axis title\") +\n  theme_NAME()\n\n\nIn our example,\n\n\nclubhouse %&gt;% ggplot(aes(x = time_with_friends,\n                         y = clubhouse_happiness)) +\n  geom_point() + \n  geom_line(aes(y = predicted_happiness_k1)) +\n  labs(x = \"Time Spent with Friends (minutes)\",\n       y = \"Clubhouse Happiness\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-13",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-13",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-14",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-14",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)\n\nNow, we can work on “prettying” up our plot.\n\nWe could also add a graph title.\n\n\n\ndataset_name %&gt;% ggplot(aes(x = variable_on_x,\n                            y = variable_on_y)) +\n  geom_TYPE() +\n  geom_TYPE(aes(y = new_y)) +\n  labs(x = \"x axis title\",\n       y = \"y axis title\",\n       title = \"title of graph\") +\n  theme_NAME()\n\n\nIn our example,\n\n\nclubhouse %&gt;% ggplot(aes(x = time_with_friends,\n                         y = clubhouse_happiness)) +\n  geom_point() + \n  geom_line(aes(y = predicted_happiness_k1)) +\n  labs(x = \"Time Spent with Friends (minutes)\",\n       y = \"Clubhouse Happiness\",\n       title = \"Predicted relationship between happiness and time spent with friends\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-15",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-15",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-16",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-16",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)\n\nNow, we can work on “prettying” up our plot.\n\nOutside of aes(), we can specify colors, line types, point shapes, etc.\n\n\n\ndataset_name %&gt;% ggplot(aes(x = variable_on_x,\n                            y = variable_on_y)) +\n  geom_TYPE(color = \"#HEX\", size = size_number) +\n  geom_TYPE(aes(y = new_y), color = \"#HEX\", size = size_number) +\n  labs(x = \"x axis title\",\n       y = \"y axis title\",\n       title = \"title of graph\") +\n  theme_NAME()\n\n\nIn our example,\n\n\nclubhouse %&gt;% ggplot(aes(x = time_with_friends,\n                         y = clubhouse_happiness)) +\n  geom_point(color = \"#009CDE\", size = 3) + \n  geom_line(aes(y = predicted_happiness_k1), color = \"#004C97\", size = 1.5) +\n  labs(x = \"Time Spent with Friends (minutes)\",\n       y = \"Clubhouse Happiness\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-17",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-17",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#example-2-multiple-regression-k-2",
    "href": "files/lectures/W1_L2_visualization.html#example-2-multiple-regression-k-2",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Example 2: Multiple Regression (k = 2)",
    "text": "Example 2: Multiple Regression (k = 2)\n\nFor our second example, let’s look at clubhouse happiness (clubhouse_happiness) as a function of time spent with friends (time_with_friends) and big, goofy laughs (goofy_laughs).\n\n\nm2 &lt;- glm(clubhouse_happiness ~ time_with_friends + goofy_laughs, \n          family = \"gaussian\",\n          data = clubhouse)\nm2 %&gt;% tidy()\n\n\n  \n\n\n\n\n\\hat{\\text{happiness}} = 39.25 + 3.06 \\text{ time} + 0.66 \\text{ laughs}"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#model-visualization-multiple-regression",
    "href": "files/lectures/W1_L2_visualization.html#model-visualization-multiple-regression",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Model Visualization: Multiple Regression",
    "text": "Model Visualization: Multiple Regression\n\nNow that there’s an additional predictor, we can’t easily visualize the model with a simple 2D scatterplot.\n\nIn theory, we could create a 3D scatterplot with a regression plane, but those are hard to read and interpret.\n\nInstead, we will visualize the relationship between y (clubhouse happiness) and x_1 (one predictor) while holding x_2 (the other predictor) constant.\nIn our example,\n\nWe will visualize the relationship between clubhouse happiness and time spent with friends.\nTime spent with friends will be on the x-axis and allowed to vary.\nWe will hold goofy laughs constant at some value.\n\nWith continuous predictors, I typically plug in the median() when drafting initial graphs for collaborators."
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#model-visualization-multiple-regression-1",
    "href": "files/lectures/W1_L2_visualization.html#model-visualization-multiple-regression-1",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Model Visualization: Multiple Regression",
    "text": "Model Visualization: Multiple Regression\n\nclubhouse &lt;- clubhouse %&gt;%\n  mutate(predicted_happiness_k2 = 39.25 + 3.06*time_with_friends + 0.66*median(goofy_laughs))"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-18",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-18",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)\n\nThen, constructing our graph,\n\n\nclubhouse %&gt;% ggplot(aes(x = time_with_friends,\n                         y = clubhouse_happiness)) +\n  geom_point(color = \"#009CDE\", size = 3) + \n  geom_line(aes(y = predicted_happiness_k2), color = \"#004C97\", size = 1.5) +\n  labs(x = \"Time Spent with Friends (minutes)\",\n       y = \"Clubhouse Happiness\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-19",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-19",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#example-3-multiple-regression-k-3",
    "href": "files/lectures/W1_L2_visualization.html#example-3-multiple-regression-k-3",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Example 3: Multiple Regression (k = 3)",
    "text": "Example 3: Multiple Regression (k = 3)\n\nFor our third example, let’s return to our full model.\nWe looked at clubhouse happiness (clubhouse_happiness) as a function of time spent with friends (time_with_friends), big, goofy laughs (goofy_laughs), and how much Donald grumbles (donald_grumbles).\n\n\nm3 &lt;- glm(clubhouse_happiness ~ time_with_friends + goofy_laughs + donald_grumbles,\n          family = \"gaussian\",\n          data = clubhouse)\nm3 %&gt;% tidy()\n\n\n  \n\n\n\n\n\\hat{\\text{happiness}} = 47.58 + 3.58 \\text{ time} + 0.66 \\text{ laughs} - 1.06 \\text{ grumbles}"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#model-visualization-multiple-regression-2",
    "href": "files/lectures/W1_L2_visualization.html#model-visualization-multiple-regression-2",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Model Visualization: Multiple Regression",
    "text": "Model Visualization: Multiple Regression\n\nIn this example, we have k=3 predictors.\n\nWe can’t easily visualize the model with a simple 2D scatterplot or even a 3D scatterplot.\n\nInstead, we will visualize the relationship between y (clubhouse happiness) and x_1 (one predictor) while holding all other x_i (the other predictors) constant.\n\nOne x_i will vary on the x-axis.\nWe will plug in plausible values for the other predictors.\n\nIn our example,\n\nWe will visualize the relationship between clubhouse happiness and time spent with friends.\nTime spent with friends will be on the x-axis and allowed to vary.\nWe will hold goofy laughs constant at some value.\nWe will also hold Donald grumbles constant at some value."
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#model-visualization-multiple-regression-3",
    "href": "files/lectures/W1_L2_visualization.html#model-visualization-multiple-regression-3",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Model Visualization: Multiple Regression",
    "text": "Model Visualization: Multiple Regression\n\nclubhouse &lt;- clubhouse %&gt;%\n  mutate(predicted_happiness_k3 = 47.58 + 3.58*time_with_friends + 0.66*median(goofy_laughs) - 1.06*median(donald_grumbles))"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-20",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-20",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)\n\nThen, constructing our graph,\n\n\nclubhouse %&gt;% ggplot(aes(x = time_with_friends,\n                         y = clubhouse_happiness)) +\n  geom_point(color = \"#009CDE\", size = 3) + \n  geom_line(aes(y = predicted_happiness_k3), color = \"#004C97\", size = 1.5) +\n  labs(x = \"Time Spent with Friends (minutes)\",\n       y = \"Clubhouse Happiness\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-21",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-21",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#lets-explore",
    "href": "files/lectures/W1_L2_visualization.html#lets-explore",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Let’s Explore…",
    "text": "Let’s Explore…\n\nHm… what if we put the three lines on top of one another? How different are the adjusted slopes?"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#example-4-multiple-regression-k-3",
    "href": "files/lectures/W1_L2_visualization.html#example-4-multiple-regression-k-3",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Example 4: Multiple Regression (k = 3)",
    "text": "Example 4: Multiple Regression (k = 3)\n\nFor our fourth example, let’s again return to our full model.\nWe looked at clubhouse happiness (clubhouse_happiness) as a function of time spent with friends (time_with_friends), big, goofy laughs (goofy_laughs), and how much Donald grumbles (donald_grumbles).\n\n\nm4 &lt;- glm(clubhouse_happiness ~ time_with_friends + goofy_laughs + donald_grumbles,\n          family = \"gaussian\",\n          data = clubhouse)\nm4 %&gt;% tidy()\n\n\n  \n\n\n\n\n\\hat{\\text{happiness}} = 47.58 + 3.58 \\text{ time} + 0.66 \\text{ laughs} - 1.06 \\text{ grumbles}"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#model-visualization-multiple-regression-4",
    "href": "files/lectures/W1_L2_visualization.html#model-visualization-multiple-regression-4",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Model Visualization: Multiple Regression",
    "text": "Model Visualization: Multiple Regression\n\nLet’s now consider the relationship between clubhouse happiness and Donald’s grumbles.\n\nDonald grumbles will be on the x-axis and allowed to vary.\nWe will hold goofy laughs constant at some value.\nWe will also hold time spent with friends constant at some value."
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#model-visualization-multiple-regression-5",
    "href": "files/lectures/W1_L2_visualization.html#model-visualization-multiple-regression-5",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Model Visualization: Multiple Regression",
    "text": "Model Visualization: Multiple Regression\n\nclubhouse &lt;- clubhouse %&gt;%\n  mutate(predicted_happiness_d = 47.58 + 3.58*median(time_with_friends) + 0.66*median(goofy_laughs) - 1.06*donald_grumbles)"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-22",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-22",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)\n\nThen, constructing our graph,\n\n\nclubhouse %&gt;% ggplot(aes(x = donald_grumbles,\n                         y = clubhouse_happiness)) +\n  geom_point(color = \"#009CDE\", size = 3) + \n  geom_line(aes(y = predicted_happiness_d), color = \"#004C97\", size = 1.5) +\n  labs(x = \"Number of Donald Grumbles\",\n       y = \"Clubhouse Happiness\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-23",
    "href": "files/lectures/W1_L2_visualization.html#plotting-using-libraryggplot2-23",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Plotting using library(ggplot2)",
    "text": "Plotting using library(ggplot2)\n\nThen, constructing our graph,"
  },
  {
    "objectID": "files/lectures/W1_L2_visualization.html#wrap-up",
    "href": "files/lectures/W1_L2_visualization.html#wrap-up",
    "title": "Visualization of the Model: Multiple Linear Regression",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nIn this lecture, we explored how to visualize simple and multiple linear regression models using the ggplot2 library.\nFor simple linear regression, we visualized the relationship between the outcome and predictor using a scatterplot and regression line.\nFor multiple linear regression, we visualized the relationship between the outcome and one predictor while holding the other predictors constant.\nEvery week, we will review model visualization.\n\nThe general ideas won’t change, but things will get tricky when we add categorical predictors and leave the normal distribution.\n\nNext lecture: Model Assumptions"
  },
  {
    "objectID": "files/lectures/W1_L2a_geom_smooth.html#introduction",
    "href": "files/lectures/W1_L2a_geom_smooth.html#introduction",
    "title": "A note about geom_smooth()",
    "section": "Introduction",
    "text": "Introduction\n\ngeom_smooth(method = \"lm\") is a convenient function in the ggplot2 library that automatically fits and adds a line of best fit to a scatterplot.\nHowever, this line is misleading for multiple linear regression models.\nThis sidebar lecture demonstrates why it is misleading."
  },
  {
    "objectID": "files/lectures/W1_L2a_geom_smooth.html#what-does-geom_smooth-do",
    "href": "files/lectures/W1_L2a_geom_smooth.html#what-does-geom_smooth-do",
    "title": "A note about geom_smooth()",
    "section": "What does geom_smooth() do?",
    "text": "What does geom_smooth() do?\n\ngeom_smooth(method = \"lm\") fits a simple linear regression model to the data in the scatterplot.\nThat is, it is graphing the line of best fit from a model with one predictor,\n\n\ny ~ x\n\n\nHowever, that does not match the model from glm() when k &gt; 1.\n\n\ny ~ x1 + x2 + ... + xk"
  },
  {
    "objectID": "files/lectures/W1_L2a_geom_smooth.html#example",
    "href": "files/lectures/W1_L2a_geom_smooth.html#example",
    "title": "A note about geom_smooth()",
    "section": "Example",
    "text": "Example\n\nLet’s examine this graphically using the example from lecture.\nRecall our simple linear regression,\n\n\n\\hat{\\text{happiness}} = 57.55 + 3.49 \\text{ time}\n\n\nand our multiple linear regression,\n\n\n\\hat{\\text{happiness}} = 47.58 + 3.58 \\text{ time} + 0.66 \\text{ laughs} - 1.06 \\text{ grumbles}"
  },
  {
    "objectID": "files/lectures/W1_L2a_geom_smooth.html#example-geom_smooth-with-simple-linear-regression",
    "href": "files/lectures/W1_L2a_geom_smooth.html#example-geom_smooth-with-simple-linear-regression",
    "title": "A note about geom_smooth()",
    "section": "Example: geom_smooth() with simple linear regression",
    "text": "Example: geom_smooth() with simple linear regression\n\nUsing geom_smooth() in the case of simple linear regression works as expected."
  },
  {
    "objectID": "files/lectures/W1_L2a_geom_smooth.html#example-geom_smooth-with-multiple-linear-regression",
    "href": "files/lectures/W1_L2a_geom_smooth.html#example-geom_smooth-with-multiple-linear-regression",
    "title": "A note about geom_smooth()",
    "section": "Example: geom_smooth() with multiple linear regression",
    "text": "Example: geom_smooth() with multiple linear regression\n\nUsing geom_smooth() in the case of multiple linear regression does not work as expected"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#introduction",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#introduction",
    "title": "Data Management for Categorical Predictors",
    "section": "Introduction",
    "text": "Introduction\n\nRecall the general linear model,  y = \\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k + \\varepsilon \nUntil now, we have discussed continuous predictors.\nNow we will introduce the use of categorical, or qualitative, predictors.\nThis means that we will include predictors that categorize the observations.\n\nWe can assign numbers to the categories, however, the numbers are nominal."
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#lecture-example-set-up",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#lecture-example-set-up",
    "title": "Data Management for Categorical Predictors",
    "section": "Lecture Example Set Up",
    "text": "Lecture Example Set Up\n\n\nThe Duckburg Department of Neighborhood Affairs (DDNA) has been collecting incident reports involving Donald Duck and his nephews (Huey, Dewey, and Louie) after a noticeable rise in household mishaps.\n\n\nduck_incidents &lt;- read_csv(\"https://raw.githubusercontent.com/samanthaseals/SDSII/refs/heads/main/files/data/lectures/W2_duck_incidents.csv\")\n\n\nIn this dataset (duck_incidents), we have access to the following from the reports:\n\nWhich nephew was involved (nephew)\nWhat kind of mischief occurred (mischief_type)\nWhere it happened (location)\nWhether Donald was present (donald_present)\nDonald’s reaction (donald_reaction)\nThe amount of sugar ingested prior to the incident (sugar_grams)\nThe estimated dollar cost of damage resulting from the incident (damage_cost)"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#lecture-example-set-up-1",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#lecture-example-set-up-1",
    "title": "Data Management for Categorical Predictors",
    "section": "Lecture Example Set Up",
    "text": "Lecture Example Set Up\n\nLooking at the dataset,\n\n\nduck_incidents %&gt;% head()"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#categorical-variables",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#categorical-variables",
    "title": "Data Management for Categorical Predictors",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\nCategorical variables can show up in datasets two ways:\n\nAs ordinal variables: there is natural order to the categories.\n\ne.g., small, medium, large; freshman, sophomore, junior, senior.\n\nAs nominal variables: there is no natural order to the categories.\n\ne.g., treatment groups A, B, C; colors red, blue, green.\n\n\nFurther, they can be stored multiple ways in a dataset:\n\nAs character/factor variables.\nAs numeric variables (e.g., 1, 2, 3 for treatment groups A, B, C).\nAs binary indicator variables (i.e., 0/1 where 1 indicates “yes” for the characteristic)."
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#exploring-categorical-variables",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#exploring-categorical-variables",
    "title": "Data Management for Categorical Predictors",
    "section": "Exploring Categorical Variables",
    "text": "Exploring Categorical Variables\n\nWhen first sitting down to examine categorical variables, I will look at frequency charts (using count() or something similar) to see what responses are possible.\n\nThis allows us to catch any typos/casing issues. e.g., stats programs will read “FL” different from “Fl” different from “fl”.\nIf there are typos, we must fix them before including this variable in analysis.\n\nWe also want to evaluate the number of responses in each category.\n\nIf there are categories with very few responses, we may want to consider condensing categories.\ne.g., if we have a variable with categories A, B, C, D, and E but only two observations in E vs. 10+ in each of the others – we should ask ourselves if we can combine E with another cateogry.\n\nNote that we can only do this when it makes sense to! Ask yourself (and your collaborator) if the categories are similar enough to combine."
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-exploring-categorical-variables",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-exploring-categorical-variables",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Exploring Categorical Variables",
    "text": "Example: Exploring Categorical Variables\n\nLet’s look at the nephew variable in our duck incident dataset.\n\n\nkable(duck_incidents %&gt;% n_pct(nephew)) \n\n\n\n\nnephew\nn (pct)\n\n\n\n\nDewey\n150 (33.3%)\n\n\nHuey\n146 (32.4%)\n\n\nLouie\n154 (34.2%)"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-exploring-categorical-variables-1",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-exploring-categorical-variables-1",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Exploring Categorical Variables",
    "text": "Example: Exploring Categorical Variables\n\nLet’s look at the mischief_type variable in our duck incident dataset.\n\n\nkable(duck_incidents %&gt;% n_pct(mischief_type)) \n\n\n\n\nmischief_type\nn (pct)\n\n\n\n\nAnimal-Related\n105 (23.3%)\n\n\nExplosive\n97 (21.6%)\n\n\nMechanical\n133 (29.6%)\n\n\nSneaking\n115 (25.6%)"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-exploring-categorical-variables-2",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-exploring-categorical-variables-2",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Exploring Categorical Variables",
    "text": "Example: Exploring Categorical Variables\n\nLet’s look at the location variable in our duck incident dataset.\n\n\nkable(duck_incidents %&gt;% n_pct(location)) \n\n\n\n\nlocation\nn (pct)\n\n\n\n\nBackyard\n115 (25.6%)\n\n\nGarage\n97 (21.6%)\n\n\nKitchen\n106 (23.6%)\n\n\nLiving Room\n132 (29.3%)"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-exploring-categorical-variables-3",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-exploring-categorical-variables-3",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Exploring Categorical Variables",
    "text": "Example: Exploring Categorical Variables\n\nLet’s look at the donald_present variable in our duck incident dataset.\n\n\nkable(duck_incidents %&gt;% n_pct(donald_present)) \n\n\n\n\ndonald_present\nn (pct)\n\n\n\n\nNo\n254 (56.4%)\n\n\nYes\n196 (43.6%)"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-exploring-categorical-variables-4",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-exploring-categorical-variables-4",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Exploring Categorical Variables",
    "text": "Example: Exploring Categorical Variables\n\nFinally, let’s look at` the donald_reaction variable in our duck incident dataset.\n\n\nkable(duck_incidents %&gt;% n_pct(donald_reaction)) \n\n\n\n\ndonald_reaction\nn (pct)\n\n\n\n\nAssigns Chores\n83 (18.4%)\n\n\nGrounds\n128 (28.4%)\n\n\nLaughs\n70 (15.6%)\n\n\nYells\n169 (37.6%)"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#categorical-variables-formatting",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#categorical-variables-formatting",
    "title": "Data Management for Categorical Predictors",
    "section": "Categorical Variables: Formatting",
    "text": "Categorical Variables: Formatting\n\nAll of the variables we just explored are stored as string variables.\n\ni.e., R sees what is stored in the column as character data – not numeric data.\n\nThere are other ways to store variables in R (and other programs).\n\nNumeric: e.g., 1, 2, 3 for treatment groups A, B, C.\nIndicator variables: e.g., 0/1 where 1 indicates “yes” for the characteristic.\nFactor: a special R data type for categorical variables."
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-factor-variables",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-factor-variables",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Factor Variables",
    "text": "Example: Factor Variables\n\nIn R, we can convert character variables to factors using the factor() function.\n\nThis permanently changes the variable type in the dataset.\nWe can check the variable type using the class() function.\n\n\n\nclass(duck_incidents$donald_present)\n\n[1] \"character\"\n\nduck_incidents &lt;- duck_incidents %&gt;%\n  mutate(donald_present = factor(donald_present))\nclass(duck_incidents$donald_present)\n\n[1] \"factor\""
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#categorical-variables-factor-variables",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#categorical-variables-factor-variables",
    "title": "Data Management for Categorical Predictors",
    "section": "Categorical Variables: Factor Variables",
    "text": "Categorical Variables: Factor Variables\n\nThe levels of the factor are stored in the variable as strings.\nWhen we include factor variables, R defaults to the “first” level as the reference group.\n\n“First” means alphabetically first for strings and numerically smallest for numbers.\n\nThere are (more than) two approaches we can take to “relevel” a variable.\n\nUse the factor() function with the levels argument to set the order of the levels.\n“Brute force” by defining a new character variable using if_else() statements and defining the levels as “1 - first category name”, “2 - second category name”, etc."
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-factor-variables-1",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-factor-variables-1",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Factor Variables",
    "text": "Example: Factor Variables\n\nIn our example,\n\n\nlevels(duck_incidents$donald_present) # default (alphabetical)\n\n[1] \"No\"  \"Yes\"\n\nduck_incidents &lt;- duck_incidents %&gt;%\n  mutate(donald_present2 = factor(donald_present,\n                                 levels = c(\"Yes\", \"No\")))\nlevels(duck_incidents$donald_present2) # specified order\n\n[1] \"Yes\" \"No\""
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-factor-variables-2",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-factor-variables-2",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Factor Variables",
    "text": "Example: Factor Variables\n\nThis ordering extends to output display,\n\n\nduck_incidents %&gt;% \n  n_pct(donald_present) # default (alphabetical)\n\n\n  \n\n\nduck_incidents %&gt;% \n  n_pct(donald_present2) # specified order"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#categorical-variables-indicator-variables",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#categorical-variables-indicator-variables",
    "title": "Data Management for Categorical Predictors",
    "section": "Categorical Variables: Indicator Variables",
    "text": "Categorical Variables: Indicator Variables\n\nWe can create indicator (or dummy) variables to include in our model.\n\nWe will create a variable for each level of our factor variable.\n\nFor a categorical (or factor) variable with c classes, we define binary indicators as follows:\n\n\nx_i =\n\\begin{cases}\n      1 & \\textnormal{if category $i$} \\\\\n      0 & \\textnormal{if another category}\n\\end{cases}\n\n\nWe will include c-1 in our models, but we create all c of them for flexibility in model specification."
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-indicator-variables-manual",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-indicator-variables-manual",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Indicator Variables (Manual)",
    "text": "Example: Indicator Variables (Manual)\n\nWe can do this manually,\n\n\nduck_incidents &lt;- duck_incidents %&gt;%\n  mutate(loc_yard = if_else(location == \"Backyard\", 1, 0),\n         loc_garage = if_else(location == \"Garage\", 1, 0),\n         loc_kitchen = if_else(location == \"Kitchen\", 1, 0),\n         loc_living = if_else(location == \"Living Room\", 1, 0))\nduck_incidents %&gt;%\n  select(location, \n         loc_yard, loc_garage, loc_kitchen, loc_living) %&gt;%\n  head()"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-indicator-variables-fastdummies",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-indicator-variables-fastdummies",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Indicator Variables (fastDummies)",
    "text": "Example: Indicator Variables (fastDummies)\n\nAlternatively, we can use the dummy_cols() function from the fastDummies package,\n\n\nduck_incidents &lt;- duck_incidents %&gt;%\n  dummy_cols(select_columns = \"location\")\nduck_incidents %&gt;%\n  select(location, \n         location_Backyard, location_Garage, location_Kitchen, `location_Living Room`) %&gt;%\n  head()"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-indicator-variables-comparison",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-indicator-variables-comparison",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Indicator Variables (Comparison)",
    "text": "Example: Indicator Variables (Comparison)\n\nThe manual approach:\n\ngives us control over the names of the new variables\nshows the logic under the hood\n\nthis definition can then be easily replicated in another software package…\n\n\nbut unfortunately is more tedious and error-prone.\n\n\n\nmutate(loc_yard = if_else(location == \"Backyard\", 1, 0),\n       loc_garage = if_else(location == \"Garage\", 1, 0),\n       loc_kitchen = if_else(location == \"Kitchen\", 1, 0),\n       loc_living = if_else(location == \"Living Room\", 1, 0))"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-indicator-variables-comparison-1",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-indicator-variables-comparison-1",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Indicator Variables (Comparison)",
    "text": "Example: Indicator Variables (Comparison)\n\nThe fastDummies approach:\n\nis quick and easy\nbut the names of the new variables can be cumbersome when the original category names are long or have spaces and/or special characters.\n\n\n\ndummy_cols(select_columns = \"location\")\n\n\nLet’s compare the code used to print the relevant variables:\n\n\n# manual creation: \nselect(location, loc_yard, loc_garage, loc_kitchen, loc_living) \n\n# using fastDummies:\nselect(location, location_Backyard, location_Garage, location_Kitchen, `location_Living Room`)"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#combining-categories",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#combining-categories",
    "title": "Data Management for Categorical Predictors",
    "section": "Combining Categories",
    "text": "Combining Categories\n\nSometimes, we will be interested in combining categories. Reasons include:\n\ncategories are similar in meaning\n\ne.g., “playground” and “park” locations\n\nsome categories have very few observations\n\ne.g., only 2 incidents occurred in the “garage” location while 30+ incidents happened in the other locations\n\n\nthere are too many categories to reasonably include in modeling\n\ne.g., states, countries, job titles, etc.\n\n\nWe can combine categories using either case_when() or if_else() statements."
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-combining-categories",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-combining-categories",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Combining Categories",
    "text": "Example: Combining Categories\n\nLet’s look at the donald_reaction variable.\n\n\nkable(duck_incidents %&gt;% n_pct(donald_reaction))\n\n\n\n\ndonald_reaction\nn (pct)\n\n\n\n\nAssigns Chores\n83 (18.4%)\n\n\nGrounds\n128 (28.4%)\n\n\nLaughs\n70 (15.6%)\n\n\nYells\n169 (37.6%)\n\n\n\n\n\n\nAlthough we have sufficient sample size in each group, let’s look at this from a different perspective: Donald took action vs. Donald did not take action.\n\n\nduck_incidents &lt;- duck_incidents %&gt;%\n  mutate(donald_action = case_when(donald_reaction %in% c(\"Assigns Chores\", \"Grounds\") ~ \"Punished\",\n                                   donald_reaction %in% c(\"Laughs\", \"Yells\") ~ \"No Punishment\"))"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-combining-categories-1",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-combining-categories-1",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Combining Categories",
    "text": "Example: Combining Categories\n\nAlthough we have sufficient sample size in each group, let’s look at this from a different perspective: Donald took action vs. Donald did not take action.\nI always look at a frequency table after combining categories to ensure everything looks correct.\nIn our example,\n\n\nkable(duck_incidents %&gt;% n_pct(donald_reaction, donald_action))\n\n\n\n\ndonald_reaction\nNo Punishment\nPunished\n\n\n\n\nAssigns Chores\n0 (0.0%)\n83 (39.3%)\n\n\nGrounds\n0 (0.0%)\n128 (60.7%)\n\n\nLaughs\n70 (29.3%)\n0 (0.0%)\n\n\nYells\n169 (70.7%)\n0 (0.0%)"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#special-case-binary-predictors",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#special-case-binary-predictors",
    "title": "Data Management for Categorical Predictors",
    "section": "Special Case: Binary Predictors",
    "text": "Special Case: Binary Predictors\n\nSo far, we have focused on categorical predictors with more than two categories.\nSometimes, we have categorical predictors that are just two categories.\n\nThe variable we just created, donald_action, is binary.\n\nIn this case, we can include the variable as a factor or as a single indicator variable.\n\nMy preference is to include binary predictors as indicators for ease."
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-binary-predictors",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-binary-predictors",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Binary Predictors",
    "text": "Example: Binary Predictors\n\nWe just created a variable that indicates if Donald punished his nephews or not,\n\n\nkable(duck_incidents %&gt;% n_pct(donald_action))\n\n\n\n\ndonald_action\nn (pct)\n\n\n\n\nNo Punishment\n239 (53.1%)\n\n\nPunished\n211 (46.9%)\n\n\n\n\n\n\nLet’s now create a binary indicator for this variable.\n\nMy approach: name the variable after the characteristic being indicated. Then I always know that 1 = the characteristic, 0 = not the characteristic."
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#example-binary-predictors-1",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#example-binary-predictors-1",
    "title": "Data Management for Categorical Predictors",
    "section": "Example: Binary Predictors",
    "text": "Example: Binary Predictors\n\nExecuting this,\n\n\nduck_incidents &lt;- duck_incidents %&gt;%\n  mutate(punished = if_else(donald_action == \"Punished\", 1, 0))\n\n\nThen, double checking our work,\n\n\nkable(duck_incidents %&gt;% n_pct(donald_action, punished))\n\n\n\n\ndonald_action\n0\n1\n\n\n\n\nNo Punishment\n239 (100.0%)\n0 (0.0%)\n\n\nPunished\n0 (0.0%)\n211 (100.0%)"
  },
  {
    "objectID": "files/lectures/W2_L1_cat_pred_vars.html#wrap-up",
    "href": "files/lectures/W2_L1_cat_pred_vars.html#wrap-up",
    "title": "Data Management for Categorical Predictors",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nIn this lecture, we have introduced the concept of categorical predictors.\nWe focused only on the data management associated with categorical predictors.\nIn the next lecture, we will focus on including categorical predictors in our linear models."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#introduction",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#introduction",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Introduction",
    "text": "Introduction\n\nRecall the general linear model,  y = \\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k + \\varepsilon \nIn the last lecture, we introduced the inclusion of categorical predictors in our model.\n\nRemember, if a categorical predictor has c classes, we include c-1 in the model.\n\nWe also discussed how to deal with categorical variables either as a factor variable or as a set of indicator variables."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#lecture-example-set-up",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#lecture-example-set-up",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Lecture Example Set Up",
    "text": "Lecture Example Set Up\n\n\nRecall our example dataset of duck-related incidents,\n\n\nduck_incidents &lt;- read_csv(\"https://raw.githubusercontent.com/samanthaseals/SDSII/refs/heads/main/files/data/lectures/W2_duck_incidents.csv\") %&gt;%\n  mutate(loc_yard = if_else(location == \"Backyard\", 1, 0),\n         loc_garage = if_else(location == \"Garage\", 1, 0),\n         loc_kitchen = if_else(location == \"Kitchen\", 1, 0),\n         loc_living = if_else(location == \"Living Room\", 1, 0))\nduck_incidents %&gt;% head()"
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#categorical-variables-in-the-model",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#categorical-variables-in-the-model",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Categorical Variables in the Model",
    "text": "Categorical Variables in the Model\n\nWe now want to discuss statistical inference of categorical predictors.\nRecall that in the case of c \\ge 3 classes, we include c-1 indicator variables in the model.\n\nHow do I determine if the predictor as a whole is statistically significant?\nHow do I determine if individual classes are statistically significant?\n\nWhat does this mean?"
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#inference-for-categorical-variables-c-ge-2",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#inference-for-categorical-variables-c-ge-2",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Inference for Categorical Variables (c \\ge 2)",
    "text": "Inference for Categorical Variables (c \\ge 2)\n\nConsider the case of a categorical predictor with c \\ge 2 classes.\nTo determine if the categorical predictor as a whole is statistically significant, we can use an ANOVA F-test (or \\chi^2 likelihood ratio test for other GzLMs).\n\nThis tests if all coefficients for the indicator variables are equal to zero.\nIf we reject this null hypothesis, we conclude that at least one class is different.\n\nThen, to determine if individual classes are statistically significant, we can look at the t-tests for each coefficient.\n\nEach t-test tests the null hypothesis that the coefficient for that class is equal to zero.\nIf we reject for a specific class, we conclude that the difference between that class and the reference class is non-zero."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#inference-for-categorical-variables-c-ge-2-1",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#inference-for-categorical-variables-c-ge-2-1",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Inference for Categorical Variables (c \\ge 2)",
    "text": "Inference for Categorical Variables (c \\ge 2)\n\nHypotheses\n\nH_0: \\ \\beta_1 = ... = \\beta_{c-1} = 0\nH_1: at least one \\beta_i \\ne 0\n\nTest Statistic and p-Value\n\nF_0 = [\\text{value from R}], p = [\\text{value from R}]\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; \\alpha = [\\text{assumed } \\alpha].\n\nConclusion/Interpretation\n\n[Reject (if p &lt; \\alpha) or fail to reject (if p \\ge \\alpha)] H_0. There [is (if reject) or is not (if FTR)] sufficient evidence to suggest that [categorical predictor] significantly predicts [outcome variable]."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#inference-for-categorical-variables-c-ge-2-r",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#inference-for-categorical-variables-c-ge-2-r",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Inference for Categorical Variables (c \\ge 2) (R)",
    "text": "Inference for Categorical Variables (c \\ge 2) (R)\n\nHow do we perform this omnibus (or global) test?\nIf we are using a factor variable,\n\n\nm &lt;- glm(y = x1 + x2 + ... + xk, family = \"gaussian\", data = dataset_name)\ncar::Anova(m, type = 3, test = \"F\")\n\n\nIf we are using indicator variables,\n\n\nfull &lt;- glm(y = x1 + x2 + ... + xk, family = \"gaussian\", data = dataset_name) # ALL predictors\nreduced &lt;- glm(y = x1 + ... + x(k-c), family = \"gaussian\", data = dataset_name) # all predictors WITHOUT indicators\nanova(reduced, full)"
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-1-one-categorical-predictor",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-1-one-categorical-predictor",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 1: One Categorical Predictor",
    "text": "Example 1: One Categorical Predictor\n\nRecall our example from the last lecture,\n\n\nm1 &lt;- glm(damage_cost ~ location, \n          family = \"gaussian\",\n          data = duck_incidents)\ntidy(m1, conf.int = TRUE)\n\n\n  \n\n\n\n\nHere, location is a factor variable with four classes.\n\nBecause there are four classes and location is a factor, we will use the car::Anova() approach."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-1-one-categorical-predictor-1",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-1-one-categorical-predictor-1",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 1: One Categorical Predictor",
    "text": "Example 1: One Categorical Predictor\n\nTo determine if location is statistically significant as a whole,\n\n\ncar::Anova(m1, type = 3, test = \"F\")\n\n\n  \n\n\n\n\nWe see that location significantly predicts damage cost (p = 0.017)."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-1-one-categorical-predictor-2",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-1-one-categorical-predictor-2",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 1: One Categorical Predictor",
    "text": "Example 1: One Categorical Predictor\n\nHypotheses\n\nH_0: \\ \\beta_{\\text{garage}} = \\beta_{\\text{kitchen}} = \\beta_{\\text{living room}} = 0\nH_1: at least one \\beta_i \\ne 0\n\nTest Statistic and p-Value\n\nF_0 = 3.450, p = 0.017\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; \\alpha = 0.05.\n\nConclusion/Interpretation\n\nReject H_0. There is sufficient evidence to suggest that location significantly predicts damage cost."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#inference-for-categorical-variables-c-ge-2-2",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#inference-for-categorical-variables-c-ge-2-2",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Inference for Categorical Variables (c \\ge 2)",
    "text": "Inference for Categorical Variables (c \\ge 2)\n\nWhen we have determined that a categorical predictor is statistically significant as a whole, we can look at the individual t-tests for each class.\nThen, our hypotheses for each class are:\n\nH_0: \\ \\beta_i = 0 (class i is not significantly different from the reference class)\nH_1: \\ \\beta_i \\ne 0 (class i is significantly different from the reference class)\n\nRemember that these are testing that particular group against the reference group and are similar to posthoc tests.\n\nNote! Because this is similar to posthoc testing, we should adjust our \\alpha.\nA simple way to do this is to use the Bonferroni correction: \\alpha_{\\text{B}} = \\frac{\\alpha}{c-1}."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#inference-for-categorical-variables-c-ge-2-3",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#inference-for-categorical-variables-c-ge-2-3",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Inference for Categorical Variables (c \\ge 2)",
    "text": "Inference for Categorical Variables (c \\ge 2)\n\nHypotheses\n\nH_0: \\ \\beta_i  = 0\nH_1: \\ \\beta_i \\ne 0\n\nTest Statistic and p-Value\n\nt_0 = [\\text{value from R}], p = [\\text{value from R}]\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; ; \\alpha = [(\\text{assumed } \\alpha)/(c-1)].\n\nConclusion/Interpretation\n\n[Reject (if p &lt; \\alpha) or fail to reject (if p \\ge \\alpha)] H_0. There [is (if reject) or is not (if FTR)] sufficient evidence to suggest that [class name] is significantly different from [reference class name] when predicting [outcome variable]."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-1-one-categorical-predictor-3",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-1-one-categorical-predictor-3",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 1: One Categorical Predictor",
    "text": "Example 1: One Categorical Predictor\n\nIn our example,\n\n\ntidy(m1, conf.int = TRUE)\n\n\n  \n\n\n\n\nThe garage is not significantly different from the backyard (p = 0.013). This difference is estimated to be $64 (-$18, $147).\nThe kitchen is significantly different from the backyard (p = 0.001). We estimate this difference to be $131 ($51, $212).\nThe living room is not significantly different from the backyard (p = 0.013). This difference is estimated to be $59 (-$17, $135)."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 2: Two Categorical Predictors",
    "text": "Example 2: Two Categorical Predictors\n\nRecall our second example,\n\n\nm4 &lt;- glm(damage_cost ~ location + nephew,\n          family = \"gaussian\",\n          data = duck_incidents)\ntidy(m4, conf.int = TRUE)\n\n\n  \n\n\n\n\nHere, we have two categorical predictors: location (4 classes) and nephew (2 classes).\n\nBecause both location is a factor variable with 4 classes, we will use the car::Anova() approach again."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-1",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-1",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 2: Two Categorical Predictors",
    "text": "Example 2: Two Categorical Predictors\n\nTo determine if location and nephew are statistically significant as a whole,\n\n\ncar::Anova(m4, type = 3, test = \"F\")\n\n\n  \n\n\n\n\nWe see that both location (p = 0.034) and nephew (p &lt; 0.001) significantly predict damage cost."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-2",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-2",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 2: Two Categorical Predictors",
    "text": "Example 2: Two Categorical Predictors\n\nWe mentioned earlier that if we are using indicator variables, we will take the reduced/full approach.\nWe can still do that here – let’s try it out.\n\n\nfull &lt;- glm(damage_cost ~ location + nephew, family = \"gaussian\",data = duck_incidents) # full model\nreduced_loc &lt;- glm(damage_cost ~ nephew, family = \"gaussian\", data = duck_incidents) # reduced without location\nreduced_neph &lt;- glm(damage_cost ~ location, family = \"gaussian\", data = duck_incidents) # reduced without nephew\nanova(reduced_loc, full) # test location\n\n\n  \n\n\nanova(reduced_neph, full) # test nephew"
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-3",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-3",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 2: Two Categorical Predictors",
    "text": "Example 2: Two Categorical Predictors\n\ncar::Anova(m4, type = 3, test = \"F\") # car::Anova() approach\n\n\n  \n\n\nanova(reduced_loc, full) # full/reduced for location\n\n\n  \n\n\nanova(reduced_neph, full) # full/reduced for nephew"
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-4",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-4",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 2: Two Categorical Predictors",
    "text": "Example 2: Two Categorical Predictors\n\nHypotheses\n\nH_0: \\ \\beta_{\\text{garage}} = \\beta_{\\text{kitchen}} = \\beta_{\\text{living room}} = 0\nH_1: at least one \\beta_i \\ne 0\n\nTest Statistic and p-Value\n\nF_0 = 2.91, p = 0.034\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; \\alpha = 0.05.\n\nConclusion/Interpretation\n\nReject H_0. There is sufficient evidence to suggest that location significantly predicts damage cost."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-5",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-5",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 2: Two Categorical Predictors",
    "text": "Example 2: Two Categorical Predictors\n\nHypotheses\n\nH_0: \\ \\beta_{\\text{Huey}} = \\beta_{\\text{Louie}} = 0\nH_1: at least one \\beta_i \\ne 0\n\nTest Statistic and p-Value\n\nF_0 = 26.69, p &lt; 0.001\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; \\alpha = 0.05.\n\nConclusion/Interpretation\n\nReject H_0. There is sufficient evidence to suggest that nephew significantly predicts damage cost."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-6",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-6",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 2: Two Categorical Predictors",
    "text": "Example 2: Two Categorical Predictors\n\nLet’s re-examine this model with the location indicators,\n\n\nm4a &lt;- glm(damage_cost ~ location + nephew, family = \"gaussian\", data = duck_incidents)\ncoefficients(m4a)\n\n        (Intercept)      locationGarage     locationKitchen locationLiving Room \n         139.288148           51.491118          113.783352           66.873723 \n         nephewHuey         nephewLouie \n          -7.974478          206.606636 \n\nm4b &lt;- glm(damage_cost ~ loc_garage + loc_kitchen + loc_living + nephew, family = \"gaussian\", data = duck_incidents)\ncoefficients(m4b)\n\n(Intercept)  loc_garage loc_kitchen  loc_living  nephewHuey nephewLouie \n 139.288148   51.491118  113.783352   66.873723   -7.974478  206.606636"
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-7",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-7",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 2: Two Categorical Predictors",
    "text": "Example 2: Two Categorical Predictors\n\nWhat happens when I run these models through car::Anova()?\n\n\ncar::Anova(m4a, type = 3, test = \"F\") # factor variable\n\n\n  \n\n\ncar::Anova(m4b, type = 3, test = \"F\") # indicator variables"
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-8",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-8",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 2: Two Categorical Predictors",
    "text": "Example 2: Two Categorical Predictors\n\nIf you have (manually) included indicator variables, you must take the full/reduced approach when examining omnibus tests for categorical predictors.\n\n\ncar::Anova(m4b, type = 3, test = \"F\") # indicator variables\n\n\n  \n\n\nreduced_loc2 &lt;- glm(damage_cost ~ nephew, data = duck_incidents) # reduced without location\nanova(reduced_loc2, m4b) # test location"
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-9",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-9",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 2: Two Categorical Predictors",
    "text": "Example 2: Two Categorical Predictors\n\nNow that we know that location and nephew are statistically significant predictors (p = 0.034 and p &lt; 0.001, respectively), we can look at the individual t-tests for each class.\n\n\ntidy(m4, conf.int = TRUE)"
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-10",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-2-two-categorical-predictors-10",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 2: Two Categorical Predictors",
    "text": "Example 2: Two Categorical Predictors\n\nLocation:\n\nThe garage is not significantly different from the backyard (p = 0.197). This difference is estimated to be $51 (-$27, $130).\nThe kitchen is significantly different from the backyard (p = 0.004). We estimate this difference to be $121 ($37, $190).\nThe living room is not significantly different from the backyard (p = 0.071). This difference is estimated to be $54 (-$5, $139).\n\nNephew\n\nThere is not a difference in cost between Huey and Dewey (p=0.813$). We estimate that Huey causes $8 less in damage (-$74, $58).\nThere is a difference between Louie and Dewey (p &lt; 0.001). We estimate that Louie causes $207 more in damage ($141, $272)."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-3-mixing-continuous-and-categorical-predictors",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-3-mixing-continuous-and-categorical-predictors",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nLet’s now at location, newphew, and sugar intake as predictors.\n\nWhat changes when continuous predictors are in the mix?\n\n\n\nm5 &lt;- glm(damage_cost ~ location + nephew + sugar_grams,\n          family = \"gaussian\",\n          data = duck_incidents)\ntidy(m5, conf.int = TRUE)"
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-3-mixing-continuous-and-categorical-predictors-1",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-3-mixing-continuous-and-categorical-predictors-1",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nThankfully, nothing new is introduced – we can take either approach!\n\nNote that when I have a factor as a predictor, I always use car::Anova().\n\n\n\ncar::Anova(m5, type = 3, test = \"F\")\n\n\n  \n\n\n\n\nWe can see that nephew is significant (p &lt; 0.001) while location and sugar intake are not (p = 0.140 and p = 0.073, respectfully).\n\nIf we were to do the pairwise testing, we would only consider the nephew comparisons.\nWe would not look at the location comparisons because location is not significant at the global level."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-3-mixing-continuous-and-categorical-predictors-2",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-3-mixing-continuous-and-categorical-predictors-2",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nWait… sugar intake can be examined using car::Anova() too?\n\n\ncar::Anova(m5, type = 3, test = \"F\")\n\n\n  \n\n\n\n\nYes! We do not need separate tidy() output to examine continuous predictors.\n\n\ntidy(m5, conf.int = TRUE) %&gt;% tail(n=4)"
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-3-mixing-continuous-and-categorical-predictors-3",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-3-mixing-continuous-and-categorical-predictors-3",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nRecall the relationship between a t test and an F test:\n\n\nF \\approx t^2\n\n\nand that their p-values will be the same.\nWe see that in this example,\n\n\n\n\nMethod\nTest Statistic\np-Value\n\n\n\n\ncar::Anova()\nF = 3.22\n0.073\n\n\ntidy()\nt = 1.80\n0.073\n\n\n\n\nTo confirm the relationship, 1.80^2 = 3.24 \\approx 3.22."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-4-binary-predictors",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-4-binary-predictors",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 4: Binary Predictors",
    "text": "Example 4: Binary Predictors\n\nRecall, we created a binary indicator for punished – it is stored as 0/1 (numeric).\n\n\nduck_incidents &lt;- duck_incidents %&gt;%\n  mutate(donald_action = case_when(donald_reaction %in% c(\"Assigns Chores\", \"Grounds\") ~ \"Punished\",\n                                   donald_reaction %in% c(\"Laughs\", \"Yells\") ~ \"No Punishment\")) %&gt;%\n  mutate(punished = if_else(donald_action == \"Punished\", 1, 0))\nduck_incidents %&gt;% count(punished)"
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-4-binary-predictors-1",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-4-binary-predictors-1",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 4: Binary Predictors",
    "text": "Example 4: Binary Predictors\n\nLet’s now model damage cost using sugar_grams and punished.\n\n\nm6 &lt;- glm(damage_cost ~ sugar_grams + punished,\n          family = \"gaussian\",\n          data = duck_incidents)\ntidy(m6, conf.int = TRUE)\n\n\n  \n\n\n\n\nBecause punished is a binary indicator, we can look at the hypothesis testing like any other continuous predictor.\nIn this model, sugar intake is significant (p &lt; 0.001) while punishment is not (p = 0.773)."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#example-4-binary-predictors-2",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#example-4-binary-predictors-2",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Example 4: Binary Predictors",
    "text": "Example 4: Binary Predictors\n\nComparing this to the car::Anova() approach,\n\n\ntidy(m6, conf.int = TRUE)\n\n\n  \n\n\ncar::Anova(m6, type = 3, test = \"F\")\n\n\n  \n\n\n\n\nThe p-values are the same.\n\nSugar intake is significant (p &lt; 0.001) while punishment is not (p = 0.773)."
  },
  {
    "objectID": "files/lectures/W2_L3_cat_pred_tests.html#summary",
    "href": "files/lectures/W2_L3_cat_pred_tests.html#summary",
    "title": "Categorical Predictors: Statistical Testing",
    "section": "Summary",
    "text": "Summary\n\nWhen dealing with categorical predictors with c \\ge 2, we must remember to test the global significance.\n\nIf using a factor variable, we can use car::Anova().\nIf using indicator variables, we must use the full/reduced approach.\n\nWhen the omnibus test is significant, we then look at the individual t-tests for each class for pairwise significance.\n\nRemember that we are always comparing against the reference group.\nIf the omnibus test is not significant, we do not consider the individual t-tests.\n\nLike with most things in R, there is more than one way to complete the analysis.\n\nChoose the method that (1) is appropriate and (2) makes the most sense to you/your brain."
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#introduction",
    "href": "files/lectures/W2_L4_visualization.html#introduction",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Introduction",
    "text": "Introduction\n\nLast week, we discussed how to visualize the resulting model when there are only continuous predictors.\nCategorical predictors change our approach with graphing because categorical predictors are inherently non-numeric while our graph axes are entirely numeric.\nThus, we are adding categorical predictors today to get an idea of how to handle them.\n\nWe will cover models with only categorical predictors and models with both categorical and continuous predictors."
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#lecture-example-set-up",
    "href": "files/lectures/W2_L4_visualization.html#lecture-example-set-up",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Lecture Example Set Up",
    "text": "Lecture Example Set Up\n\n\nRecall our example dataset of duck-related incidents,\n\n\nduck_incidents &lt;- read_csv(\"https://raw.githubusercontent.com/samanthaseals/SDSII/refs/heads/main/files/data/lectures/W2_duck_incidents.csv\") %&gt;%\n  mutate(loc_yard = if_else(location == \"Backyard\", 1, 0),\n         loc_garage = if_else(location == \"Garage\", 1, 0),\n         loc_kitchen = if_else(location == \"Kitchen\", 1, 0),\n         loc_living = if_else(location == \"Living Room\", 1, 0))\nduck_incidents %&gt;% head()"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-1-one-categorical-predictor",
    "href": "files/lectures/W2_L4_visualization.html#example-1-one-categorical-predictor",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 1: One Categorical Predictor",
    "text": "Example 1: One Categorical Predictor\n\nRecall our example from the last lectures,\n\n\nm1 &lt;- glm(damage_cost ~ location, \n          data = duck_incidents)\ntidy(m1, conf.int = TRUE)"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-1-one-categorical-predictor-1",
    "href": "files/lectures/W2_L4_visualization.html#example-1-one-categorical-predictor-1",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 1: One Categorical Predictor",
    "text": "Example 1: One Categorical Predictor\n\nThis model only has categorical predictors that have no order to them.\n\n\n\\hat{\\text{cost}} = 202.78 + 64.34 \\text{ garage} + 131.85 \\text{ kitchen} + 58.70 \\text{ living}\n\n\nBecause this is effectively a simple linear regression (one predictor with 4 levels - location), the way we visualized the model before does not make sense.\n\nWhy? We do not have a variable to put on the x-axis.\n\nInstead, we will use a different type of plot to visualize the results.\n\nIn this case, I would graph the predicted damage cost for each location and include error bars for the confidence intervals.\n\nFirst, we need to find the marginal means and the simultaneous confidence intervals."
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#side-note-marginal-means-simultaneous-ci",
    "href": "files/lectures/W2_L4_visualization.html#side-note-marginal-means-simultaneous-ci",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Side Note: Marginal Means & Simultaneous CI",
    "text": "Side Note: Marginal Means & Simultaneous CI\n\nWhat are marginal means and simultaneous confidence intervals?\nMarginal means are the predicted means for each level of a categorical predictor, averaging (or “marginalizing”) over the other predictors in the model.\nSimultaneous confidence intervals are confidence intervals that account for multiple comparisons, ensuring that the overall confidence level is maintained across all intervals.\n\nThat is, we know that together, the condfidence intervals have an overall 95% confidence level.\n\nThis means that the resulting CI will depend on the number of comparisons we are performing.\n\nWhen using the Bonferroni correction, we only divide by the number of comparisons we are interested in.\nThe simultaneous CI accounts for all possible comparisons."
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#side-note-marginal-means-simultaneous-ci-r",
    "href": "files/lectures/W2_L4_visualization.html#side-note-marginal-means-simultaneous-ci-r",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Side Note: Marginal Means & Simultaneous CI (R)",
    "text": "Side Note: Marginal Means & Simultaneous CI (R)\n\nIn R, we will use the emmeans() function from the emmeans package to calculate the marginal means.\n\n\nemm &lt;- emmeans(model_name, ~ categorical_variable) \n\n\nThen, we use the confint() function to calculate the simultaneous confidence intervals.\n\n\nemm_ci &lt;- confint(emm, adjust = \"sidak\") \n\n\nFinally, we can create the dataset for graphing.\n\n\nconf_data &lt;- as_tibble(emm_ci)"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-1-one-categorical-predictor-2",
    "href": "files/lectures/W2_L4_visualization.html#example-1-one-categorical-predictor-2",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 1: One Categorical Predictor",
    "text": "Example 1: One Categorical Predictor\n\nIn our example,\n\n\nemm &lt;- emmeans(m1, ~ location) \nemm_ci &lt;- confint(emm, adjust = \"sidak\") \ngraph &lt;- as_tibble(emm_ci)\n\n\nLooking at the resulting data,\n\n\n\n\n\n\nLocation\nAverage Cost\nLCL\nUCL\n\n\n\n\nBackyard\n202.7798\n131.7132\n273.8465\n\n\nGarage\n267.1216\n189.7416\n344.5017\n\n\nKitchen\n334.6267\n260.6045\n408.6489\n\n\nLiving Room\n261.4766\n195.1439\n327.8093"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#graphing-means-and-simultaneous-cis",
    "href": "files/lectures/W2_L4_visualization.html#graphing-means-and-simultaneous-cis",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Graphing Means and Simultaneous CIs",
    "text": "Graphing Means and Simultaneous CIs\n\nNow that our dataset has been created, we can construct the graph.\n\n\ngraph %&gt;% ggplot(aes(x = categorical_variable, y = emmean)) +\n    geom_point() +\n    geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), width = 0.2) +\n    labs(x = \"Categorical Label\",\n         y = \"Outcome Label\") +\n    theme_bw()"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-1-one-categorical-predictor-3",
    "href": "files/lectures/W2_L4_visualization.html#example-1-one-categorical-predictor-3",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 1: One Categorical Predictor",
    "text": "Example 1: One Categorical Predictor\n\nIn our example,\n\n\ngraph %&gt;% ggplot(aes(x = location, y = emmean)) +\n    geom_point() +\n    geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), width = 0.2) +\n    labs(x = \"Location\",\n         y = \"Average Damage Cost\") +\n    theme_bw()"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nRecall the model examining damage cost as a function of location, nephew, and sugar_grams.\n\n\nm5 &lt;- glm(damage_cost ~ location + nephew + sugar_grams,\n          data = duck_incidents)\ntidy(m5, conf.int = TRUE)"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#visualizing-models-with-mixed-predictor-types",
    "href": "files/lectures/W2_L4_visualization.html#visualizing-models-with-mixed-predictor-types",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Visualizing Models with Mixed Predictor Types",
    "text": "Visualizing Models with Mixed Predictor Types\n\nNow we have a model with one continuous predictor and two categorical predictors.\nThis means that we can return to our previous method of visualizing the model.\n\nWe will graph the predicted damage cost (y) as a function of sugar grams (x); our predicted values will depend on location and nephew.\n\nHow do we include categorical predictors in this graph? There are two approaches:\n\nLook at individual graphs for different categories.\nCreate different lines for specific categories."
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-1",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-1",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nOur current model has both categorical and continuous predictors,\n\n\n\\begin{align*}\n\\hat{\\text{cost}} = 62.16 &+ 54.77 \\text{ garage} + 92.65 \\text{ kitchen} + 58.37 \\text{ living} \\\\\n& - 0.90 \\text{ Huey} + 198.79 \\text{ Louie} \\\\\n& + 1.84 \\text{ sugar}\n\\end{align*}\n\n\nIn our example,\n\nWe could create four graphs: one for each location with the lines defined by nephew.\nWe could create three graphs: one for each nephew with the lines defined by location.\nWe could create a single graph with lines defined by both location and nephew.\netc."
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#visualizing-models-with-mixed-predictor-types-1",
    "href": "files/lectures/W2_L4_visualization.html#visualizing-models-with-mixed-predictor-types-1",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Visualizing Models with Mixed Predictor Types",
    "text": "Visualizing Models with Mixed Predictor Types\n\nWait! There are multiple approaches!\nWhich approach is appropriate?\n\nIt depends on the context of the problem and what comparisons are most relevant.\n\nNote: I do not put a ton of effort on graphs when drafting them for collaborators. I will draft something basic for demonstration purposes and discussion.\n\ne.g., maybe a graph for a single nephew or a single location, then ask which is preferred before I draft the graphs for the additional nephews or locations.\n\n\nFor class credit: I am looking for competency in graphing. I am not looking for perfect graphs. Remember that we are practicing and cannot talk to our (hypothetical) collaborators in our projects."
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#creating-predicted-values-with-categorical-predictors",
    "href": "files/lectures/W2_L4_visualization.html#creating-predicted-values-with-categorical-predictors",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Creating Predicted Values with Categorical Predictors",
    "text": "Creating Predicted Values with Categorical Predictors\n\nRecall how we defined our indicator variables for nephew,\n\n\n\n\nNephew\nx_{\\text{H}}\nx_{\\text{D}}\nx_{\\text{L}}\n\n\n\n\nHuey\n1\n0\n0\n\n\nDewey\n0\n1\n0\n\n\nLouie\n0\n0\n1\n\n\n\n\nThat means, to create predicted values for specific nephews, we need to set the indicator variables accordingly.\n\ne.g., for Huey, set x_{\\text{H}} = 1, x_{\\text{D}} = 0, and x_{\\text{L}} = 0.\ne.g., for Dewey, set x_{\\text{H}} = 0, x_{\\text{D}} = 1, and x_{\\text{L}} = 0.\ne.g., for Louie, set x_{\\text{H}} = 0, x_{\\text{D}} = 0, and x_{\\text{L}} = 1.\n\nRemember, we only plug in for two of the newphews in the model (the third is the reference category)."
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#creating-predicted-values-with-categorical-predictors-1",
    "href": "files/lectures/W2_L4_visualization.html#creating-predicted-values-with-categorical-predictors-1",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Creating Predicted Values with Categorical Predictors",
    "text": "Creating Predicted Values with Categorical Predictors\n\nWe will take the same approach for location.\n\n\n\n\n\n\n\n\n\n\n\nLocation\nx_{\\text{B}}\nx_{\\text{G}}\nx_{\\text{K}}\nx_{\\text{L}}\n\n\n\n\nBackyard\n1\n0\n0\n0\n\n\nGarage\n0\n1\n0\n0\n\n\nKitchen\n0\n0\n1\n0\n\n\nLiving Room\n0\n0\n0\n1\n\n\n\n\nThen, the indicators will be plugged in as follows,\n\ne.g., for backyard, set x_{\\text{B}} = 1, x_{\\text{G}} = 0, x_{\\text{K}} = 0, and x_{\\text{L}} = 0.\ne.g., for garage, set x_{\\text{B}} = 0, x_{\\text{G}} = 1, x_{\\text{K}} = 0, and x_{\\text{L}} = 0.\ne.g., for kitchen, set x_{\\text{B}} = 0, x_{\\text{G}} = 0, x_{\\text{K}} = 1, and x_{\\text{L}} = 0.\ne.g., for living room, set set x_{\\text{B}} = 0, x_{\\text{G}} = 0, x_{\\text{K}} = 0, and x_{\\text{L}} = 1."
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-2",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-2",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nBack to the current model,\n\n\n\\begin{align*}\n\\hat{\\text{cost}} = 62.16 &+ 54.77 \\text{ garage} + 92.65 \\text{ kitchen} + 58.37 \\text{ living} \\\\\n& - 0.90 \\text{ Huey} + 198.79 \\text{ Louie} \\\\\n& + 1.84 \\text{ sugar}\n\\end{align*}\n\n\nLet’s first create what I would call the “draft” for my collaborator, then we will create the full graph.\nWe will create the graphs for the nephews and let the lines define the location.\nThat means sugar will vary on the x-axis."
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-3",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-3",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nDefining our predicted values, we will need one for each location (because they are defining our lines) and for each nephew (because they are defining our graphs).\n\n\n\\begin{align*}\n\\hat{\\text{cost}} = 62.16 &+ 54.77 \\text{ g} + 92.65 \\text{ k} + 58.37 \\text{ lr} - 0.90 \\text{ H} + 198.79 \\text{ L} + 1.84 \\text{ sug.}\n\\end{align*}\n\n\nFor Huey,\n\n\n\n\nLocation\nEquation for Predicted Cost\n\n\n\n\nBackyard\ncostH = 62.16 - 0.90 + 1.84 (sugar)\n\n\nGarage\ncostH = 62.16 + 54.77 - 0.90 + 1.84 (sugar)\n\n\nKitchen\ncostH = 62.16 + 92.65 - 0.90 + 1.84 (sugar)\n\n\nLiving Room\ncostH = 62.16 + 58.37 - 0.90 + 1.84 (sugar)"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-4",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-4",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nDefining our predicted values, we will need one for each location (because they are defining our lines) and for each nephew (because they are defining our graphs).\n\n\n\\begin{align*}\n\\hat{\\text{cost}} = 62.16 &+ 54.77 \\text{ g} + 92.65 \\text{ k} + 58.37 \\text{ lr} - 0.90 \\text{ H} + 198.79 \\text{ L} + 1.84 \\text{ sug.}\n\\end{align*}\n\n\nFor Dewey,\n\n\n\n\nLocation\nEquation for Predicted Cost\n\n\n\n\nBackyard\ncostD = 62.16 + 1.84 (sugar)\n\n\nGarage\ncostD = 62.16 + 54.77 + 1.84 (sugar)\n\n\nKitchen\ncostD = 62.16 + 92.65 + 1.84 (sugar)\n\n\nLiving Room\ncostD = 62.16 + 58.37 + 1.84 (sugar)"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-5",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-5",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nDefining our predicted values, we will need one for each location (because they are defining our lines) and for each nephew (because they are defining our graphs).\n\n\n\\begin{align*}\n\\hat{\\text{cost}} = 62.16 &+ 54.77 \\text{ g} + 92.65 \\text{ k} + 58.37 \\text{ lr} - 0.90 \\text{ H} + 198.79 \\text{ L} + 1.84 \\text{ sug.}\n\\end{align*}\n\n\nFor Louie,\n\n\n\n\n\n\n\n\nLocation\nEquation for Predicted Cost\n\n\n\n\nBackyard\ncostL = 62.16 + 198.79 + 1.84 (sugar)\n\n\nGarage\ncostL = 62.16 + 54.77 + 198.79 + 1.84 (sugar)\n\n\nKitchen\ncostL = 62.16 + 92.65 + 198.79 + 1.84 (sugar)\n\n\nLiving Room\ncostL = 62.16 + 58.37 + 198.79 + 1.84 (sugar)"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-6",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-6",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nYes… unfortunately this is: a lot of equations \\to a lot of coding \\to why we start with a draft.\nBut, for fun, let’s get detailed. First, our predicted values.\n\n\nduck_incidents &lt;- duck_incidents %&gt;%\n  mutate(pred_H_bk = 62.16 - 0.90 + 1.84 * sugar_grams,\n         pred_H_gar = 62.16 + 54.77 - 0.90 + 1.84 * sugar_grams,\n         pred_H_kit = 62.16 + 92.65 - 0.90 + 1.84 * sugar_grams,\n         pred_H_lr = 62.16 + 58.37 - 0.90 + 1.84 * sugar_grams,\n         pred_D_bk = 62.16 + 1.84 * sugar_grams,\n         pred_D_gar = 62.16 + 54.77 + 1.84 * sugar_grams,\n         pred_D_kit = 62.16 + 92.65 + 1.84 * sugar_grams,\n         pred_D_lr = 62.16 + 58.37 + 1.84 * sugar_grams,\n         pred_L_bk = 62.16 + 198.79 + 1.84 * sugar_grams,\n         pred_L_gar = 62.16 + 54.77 + 198.79 + 1.84 * sugar_grams,\n         pred_L_kit = 62.16 + 92.65 + 198.79 + 1.84 * sugar_grams,\n         pred_L_lr = 62.16 + 58.37 + 198.79 + 1.84 * sugar_grams)"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-7",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-7",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nNow, graphs for each nephew.\n\n\ng_H &lt;- duck_incidents %&gt;%\n  ggplot(aes(x = sugar_grams, y = damage_cost)) +\n  geom_point(data = filter(duck_incidents, nephew == \"Huey\")) +\n  geom_line(aes(y = pred_H_bk, color = \"Backyard\")) +\n  geom_line(aes(y = pred_H_gar, color = \"Garage\")) +\n  geom_line(aes(y = pred_H_kit, color = \"Kitchen\")) +\n  geom_line(aes(y = pred_H_lr, color = \"Living Room\")) +\n  labs(x = \"Sugar (grams)\",\n       y = \"Predicted Damage Cost\",\n       title = \"Huey\") +\n  scale_color_discrete(name = \"Location\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-8",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-8",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nNow, graphs for each nephew.\n\n\ng_D &lt;- duck_incidents %&gt;%\n  ggplot(aes(x = sugar_grams, y = damage_cost)) +\n  geom_point(data = filter(duck_incidents, nephew == \"Dewey\")) +\n  geom_line(aes(y = pred_D_bk, color = \"Backyard\")) +\n  geom_line(aes(y = pred_D_gar, color = \"Garage\")) +\n  geom_line(aes(y = pred_D_kit, color = \"Kitchen\")) +\n  geom_line(aes(y = pred_D_lr, color = \"Living Room\")) +\n  labs(x = \"Sugar (grams)\",\n       y = \"Predicted Damage Cost\",\n       title = \"Dewey\") +\n  scale_color_discrete(name = \"Location\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-9",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-9",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nNow, graphs for each nephew.\n\n\ng_L &lt;- duck_incidents %&gt;%\n  ggplot(aes(x = sugar_grams, y = damage_cost)) +\n  geom_point(data = filter(duck_incidents, nephew == \"Louie\")) +\n  geom_line(aes(y = pred_L_bk, color = \"Backyard\")) +\n  geom_line(aes(y = pred_L_gar, color = \"Garage\")) +\n  geom_line(aes(y = pred_L_kit, color = \"Kitchen\")) +\n  geom_line(aes(y = pred_L_lr, color = \"Living Room\")) +\n  labs(x = \"Sugar (grams)\",\n       y = \"Predicted Damage Cost\",\n       title = \"Louie\") +\n  scale_color_discrete(name = \"Location\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-10",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-10",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nFinally, we can display the graphs together."
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-11",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-11",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nMaking some edits (see .qmd file),"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-12",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-12",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nWe could also create a graph for each location with lines defined by nephew. For example,\n\n\ng_kit &lt;- duck_incidents %&gt;%\n  ggplot(aes(x = sugar_grams, y = damage_cost)) +\n  geom_point(data = filter(duck_incidents, location == \"Kitchen\")) +\n  geom_line(aes(y = pred_H_kit, color = \"Huey\")) +\n  geom_line(aes(y = pred_D_kit, color = \"Dewey\")) +\n  geom_line(aes(y = pred_L_kit, color = \"Louie\")) +\n  labs(x = \"\",\n       y = \"Predicted Damage Cost\",\n       title = \"Kitchen\") +\n  ylim(0, 2750) +\n  scale_color_discrete(name = \"Nephew\") +\n  theme_bw() + \n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-13",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-13",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nSimilar to our last example,"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-14",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-14",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nWait… there are three nephews…"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-15",
    "href": "files/lectures/W2_L4_visualization.html#example-3-mixing-continuous-and-categorical-predictors-15",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nLet’s take a peek at the data…\n\n\nduck_incidents %&gt;%\n  select(pred_H_kit, pred_D_kit, pred_L_kit) %&gt;%\n  head()\n\n\n  \n\n\n\n\nAh… Huey and Dewey are similar while Louie causes much more damage than the other two."
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#using-graphs-to-explain-model-results",
    "href": "files/lectures/W2_L4_visualization.html#using-graphs-to-explain-model-results",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Using Graphs to Explain Model Results",
    "text": "Using Graphs to Explain Model Results\n\nWhen presenting model results, graphs can be help demonstrate what your math is showing.\n\ne.g., Louie causes much more damage than the other two nephews, especially as sugar intake increases. It is also difficult to predict the amount of damage Louie will cause because of the large variability in his damage costs.\n\nWe can also use graphs to help us answer questions about the data.\n\ne.g., Is there a location that has higher damage costs than the others?\ne.g., Is there a nephew that causes significantly more damage than the others?"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#using-graphs-to-explain-model-results-1",
    "href": "files/lectures/W2_L4_visualization.html#using-graphs-to-explain-model-results-1",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Using Graphs to Explain Model Results",
    "text": "Using Graphs to Explain Model Results\n\nRecall the analysis from last lecture,\n\n\nm5 &lt;- glm(damage_cost ~ location + nephew + sugar_grams,\n          family = \"gaussian\",\n          data = duck_incidents)\ntidy(m5, conf.int = TRUE)"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#using-graphs-to-explain-model-results-2",
    "href": "files/lectures/W2_L4_visualization.html#using-graphs-to-explain-model-results-2",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Using Graphs to Explain Model Results",
    "text": "Using Graphs to Explain Model Results\n\nRecall the analysis from last lecture,\n\n\ncar::Anova(m5, type = 3, test = \"F\")"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#using-graphs-to-explain-model-results-3",
    "href": "files/lectures/W2_L4_visualization.html#using-graphs-to-explain-model-results-3",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Using Graphs to Explain Model Results",
    "text": "Using Graphs to Explain Model Results\n\nRevisiting the graph,"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#using-graphs-to-explain-model-results-4",
    "href": "files/lectures/W2_L4_visualization.html#using-graphs-to-explain-model-results-4",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Using Graphs to Explain Model Results",
    "text": "Using Graphs to Explain Model Results\n\nWhen we initally graphed this, we saw that Huey and Dewey caused similar amounts of damage while Louie causes much more damage than the other two.\nWe can formally test the pairwise comparisons and quantify the differences,\n\n\npairs(emmeans(m5, ~ nephew), adjust = \"tukey\")\n\n contrast      estimate   SE  df t.ratio p.value\n Dewey - Huey     0.899 33.8 443   0.027  0.9996\n Dewey - Louie -198.792 33.5 443  -5.939 &lt;0.0001\n Huey - Louie  -199.691 34.4 443  -5.805 &lt;0.0001\n\nResults are averaged over the levels of: location \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\nNote that an alternative to this method would be to change the reference category and re-run the model multiple times.\nRemember, we need to adjust \\alpha for multiple comparisons!"
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#using-graphs-to-explain-model-results-5",
    "href": "files/lectures/W2_L4_visualization.html#using-graphs-to-explain-model-results-5",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Using Graphs to Explain Model Results",
    "text": "Using Graphs to Explain Model Results\n\n\nOur graph visualizes the differences we see in the pairwise comparisons.\n\nLouie causes significantly more damage than Huey and Dewey.\nHuey and Dewey cause similar amounts of damage.\n\nThey are so similar that their regression lines appear overlaid."
  },
  {
    "objectID": "files/lectures/W2_L4_visualization.html#wrap-up",
    "href": "files/lectures/W2_L4_visualization.html#wrap-up",
    "title": "Visualization of the Model: Categorical Predictors",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nIn this lecture, we discussed how to visualize models with categorical predictors.\n\nWhat we have previously discussed about visualizing models with continuous predictors still applies.\n\nThe examples I am presenting are examples – not the one and only way to visualize these models.\nAlways consider the context of the problem and what comparisons are most relevant when deciding how to visualize your model.\n\nWho is your audience?\nWhat level of detail do they need?\n\nNext week, we will discuss complicating our models further with interaction terms."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#introduction",
    "href": "files/lectures/W1_L1_review.html#introduction",
    "title": "Review of Linear Regression",
    "section": "Introduction",
    "text": "Introduction\n\nRecall the general linear model, y = \\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k\nThis is a multiple regression model because it has multiple predictors (x_i).\n\nA special case is simple linear regression, when there is a single predictor. y = \\beta_0 + \\beta_1 x_1 \n\n\\beta_0 is the y-intercept, or the average outcome (y) when all x_i = 0.\n\\beta_i is the slope for predictor i and describes the relationship between the predictor and the outcome, after adjusting (or accounting) for the other predictors in the model."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#lecture-example-set-up",
    "href": "files/lectures/W1_L1_review.html#lecture-example-set-up",
    "title": "Review of Linear Regression",
    "section": "Lecture Example Set Up",
    "text": "Lecture Example Set Up\n\nOn a busy day at the clubhouse, Mickey Mouse wants to understand what drives “happiness” at the end of the day. For each day, he records (in the clubhouse dataset):\n\nTime with friends (in hours; time_with_friends): how many hours Mickey spends hanging out with his friends.\nGoofy Laughs (a count; goofy_laughs) – how many big goofy laughs happen that day.\nDonald Grumbles (a count; donald_grumbles): how many times Donald gets frustrated and grumbles.\nClubhouse Happiness (a score; clubhouse_happiness): an overall happiness score at the end of the day.\n\nIn this lecture, we will use a linear model to explore the relationships between happiness and laughs, grumbles, and time spent with friends."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#lecture-example-set-up-1",
    "href": "files/lectures/W1_L1_review.html#lecture-example-set-up-1",
    "title": "Review of Linear Regression",
    "section": "Lecture Example Set Up",
    "text": "Lecture Example Set Up\n\n\nLet’s pull in our data,\n\n\n\n\nclubhouse &lt;- read_csv(\"https://raw.githubusercontent.com/samanthaseals/SDSII/refs/heads/main/files/data/lectures/W1_mickey_clubhouse.csv\")\nhead(clubhouse)"
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#constructing-the-linear-model-r",
    "href": "files/lectures/W1_L1_review.html#constructing-the-linear-model-r",
    "title": "Review of Linear Regression",
    "section": "Constructing the Linear Model (R)",
    "text": "Constructing the Linear Model (R)\n\nWe will use the glm() function to construct our linear model.\n\nglm() stands for Generalized Linear Model.\nFor linear regression, we will specify family = gaussian.\n\n\n\n\nm &lt;- glm(y ~ x_1 + x_2 + ... + x_k,\n         family = \"gaussian\",\n         data = dataset_name)"
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#constructing-the-linear-model",
    "href": "files/lectures/W1_L1_review.html#constructing-the-linear-model",
    "title": "Review of Linear Regression",
    "section": "Constructing the Linear Model",
    "text": "Constructing the Linear Model\n\nIn our example,\n\n\nm &lt;- glm(clubhouse_happiness ~ time_with_friends + goofy_laughs + donald_grumbles,\n         family = gaussian,\n         data = clubhouse)\ntidy(m)\n\n\n  \n\n\n\n\n\\begin{align*}\n\\hat{y} &= 47.58 + 3.58 \\ x_1 + 0.65 \\ x_2 - 1.06 \\ x_3 \\\\\n\\hat{\\text{happiness}} &= 47.58 + 3.58 \\text{ time} + 0.65 \\text{ laughs} - 1.06 \\text{ grumbles}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#interpretation-of-slope",
    "href": "files/lectures/W1_L1_review.html#interpretation-of-slope",
    "title": "Review of Linear Regression",
    "section": "Interpretation of Slope",
    "text": "Interpretation of Slope\n\nWe want to put the slope into perspective for whoever we are collaborating with.\nBasic interpretation: for every 1 [units of x_i] increase in [x_i], [y] [increases or decreases] by \\left[ \\left| \\hat{\\beta}_i \\right| \\right] [units of y].\n\nWe say that y is decreasing if \\hat{\\beta}_i &lt; 0 and y is increasing if \\hat{\\beta}_i &gt; 0.\n\nNote that in the case of multiple regression, there is an unspoken “after adjusting for everything else in the model” at the end of the sentence.\n\nAlways remember that we are looking at the relationship between y and x_i after adjusting for all other predictors included in the predictor set."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#interpretation-of-slope-1",
    "href": "files/lectures/W1_L1_review.html#interpretation-of-slope-1",
    "title": "Review of Linear Regression",
    "section": "Interpretation of Slope",
    "text": "Interpretation of Slope\n\nLet’s interpret the slopes for the model we constructed.\n\n\nm %&gt;% tidy()"
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#interpretation-of-slope-2",
    "href": "files/lectures/W1_L1_review.html#interpretation-of-slope-2",
    "title": "Review of Linear Regression",
    "section": "Interpretation of Slope",
    "text": "Interpretation of Slope\n\nLet’s interpret the slopes for the model we constructed.\n\n\n\n\n\ntime_with_friends      goofy_laughs   donald_grumbles \n        3.5760335         0.6594346        -1.0632922 \n\n\n\nFor every 1 hour increase in time spent with friends, clubhouse happiness increases by 3.58 points, after adjusting for goofy laughs and Donald grumbles.\nFor every 1 big goofy laugh increase, clubhouse happiness increases by 0.65 points, after adjusting for time spent with friends and Donald grumbles.\nFor every 1 Donald grumble increase, clubhouse happiness decreases by 1.06 points, after adjusting for time spent with friends and goofy laughs."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#interpretation-of-slope-3",
    "href": "files/lectures/W1_L1_review.html#interpretation-of-slope-3",
    "title": "Review of Linear Regression",
    "section": "Interpretation of Slope",
    "text": "Interpretation of Slope\n\nWe can also scale our interpretations. e.g.,\n\nFor every k [units of x_i] increase in [x_i], [y] [increases or decreases] by \\left[ k \\times \\left| \\hat{\\beta}_i \\right| \\right] [units of y].\n\nIn our example,\n\nFor every increase of 1 Donald grumble, clubhouse happiness decreases by 1.06 points, after adjusting for time spent with friends and goofy laughs.\nFor every increase of 5 Donald grumbles, clubhouse happiness decreases by 5.3 points, after adjusting for time spent with friends and goofy laughs."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#interpretation-of-intercept",
    "href": "files/lectures/W1_L1_review.html#interpretation-of-intercept",
    "title": "Review of Linear Regression",
    "section": "Interpretation of Intercept",
    "text": "Interpretation of Intercept\n\nThe intercept is the average of the outcome when all predictors in the model are equal to 0.\n\nWe can think of this as the “baseline” level of the outcome before any predictors have an effect.\n\nIn our example,\n\n\n\n\n\n(Intercept) \n   47.57612 \n\n\n\nThe average clubhouse happiness when time with friends = 0 hours, goofy laughs = 0, and Donald grumbles = 0 is 47.58 points."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#confidence-intervals-for-beta_i-r",
    "href": "files/lectures/W1_L1_review.html#confidence-intervals-for-beta_i-r",
    "title": "Review of Linear Regression",
    "section": "Confidence Intervals for \\beta_i (R)",
    "text": "Confidence Intervals for \\beta_i (R)\n\nRecall confidence intervals – they allow us to determine how “good” our estimation is.\nIn general, CIs will take the form\n\npoint estimate \\pm margin of error\n\nThe margin of error is a critical value (e.g., t_{1-\\alpha/2}) multiplied by the standard error of the point estimate.\nRecall that the standard error accounts for the sample size.\nIn R, we will run the model results through the tidy() function, but ask for the confidence intervals.\n\n\n\nm %&gt;% tidy(conf.int = TRUE)"
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#confidence-intervals-for-beta_i",
    "href": "files/lectures/W1_L1_review.html#confidence-intervals-for-beta_i",
    "title": "Review of Linear Regression",
    "section": "Confidence Intervals for \\beta_i",
    "text": "Confidence Intervals for \\beta_i\n\nIn our example,\n\n\nm %&gt;% tidy(conf.int = TRUE)\n\n\n  \n\n\n\n\nWe have the following CIs:\n\n95% CI for \\beta_{\\text{time}} is (2.37, 4.78)\n95% CI for \\beta_{\\text{goofy}} is (0.52, 0.79)\n95% CI for \\beta_{\\text{Donald}} is (-1.34, -0.79)"
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#confidence-intervals-for-beta_i-1",
    "href": "files/lectures/W1_L1_review.html#confidence-intervals-for-beta_i-1",
    "title": "Review of Linear Regression",
    "section": "Confidence Intervals for \\beta_i",
    "text": "Confidence Intervals for \\beta_i\n\nWhat if we want something other than a 95% CI?\n\n\n\nm %&gt;% tidy(conf.int = TRUE, conf.level = insert_level_here)"
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#confidence-intervals-for-beta_i-2",
    "href": "files/lectures/W1_L1_review.html#confidence-intervals-for-beta_i-2",
    "title": "Review of Linear Regression",
    "section": "Confidence Intervals for \\beta_i",
    "text": "Confidence Intervals for \\beta_i\n\nIn our example,\n\n\nm %&gt;% tidy(conf.int = TRUE, conf.level = 0.99)\n\n\n  \n\n\n\n\nWe have the following CIs:\n\n99% CI for \\beta_{\\text{time}} is (1.99, 5.16)\n99% CI for \\beta_{\\text{goofy}} is (0.48, 0.84)\n99% CI for \\beta_{\\text{Donald}} is (-1.42, -0.71)"
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#confidence-intervals-for-beta_i-3",
    "href": "files/lectures/W1_L1_review.html#confidence-intervals-for-beta_i-3",
    "title": "Review of Linear Regression",
    "section": "Confidence Intervals for \\beta_i",
    "text": "Confidence Intervals for \\beta_i\n\nLet’s compare the two sets of CIs.\n95% CIs for \\beta_i:\n\n95% CI for \\beta_{\\text{time}} is (2.37, 4.78)\n95% CI for \\beta_{\\text{goofy}} is (0.52, 0.79)\n95% CI for \\beta_{\\text{Donald}} is (-1.34, -0.79)\n\n99% CIs for \\beta_i:\n\n99% CI for \\beta_{\\text{time}} is (1.99, 5.16)\n99% CI for \\beta_{\\text{goofy}} is (0.48, 0.84)\n99% CI for \\beta_{\\text{Donald}} is (-1.42, -0.71)"
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#significant-regression-line",
    "href": "files/lectures/W1_L1_review.html#significant-regression-line",
    "title": "Review of Linear Regression",
    "section": "Significant Regression Line",
    "text": "Significant Regression Line\n\nWe now will ask ourselves if any of our slopes are flat.\n\n\nH_0: \\ \\beta_1 = \\beta_2 = ... = \\beta_k = 0\n\n\nWhat does it mean if the slopes are flat?\n\nIf a slope is flat, it means that there is no relationship between that predictor and the outcome, after adjusting for the other predictors in the model.\n\nWe will use hypothesis testing to determine if at least one slope is non-zero.\n\nThis is often referred to as the omnibus F-test in linear regression.\nWe will instead use the likelihood ratio test.\n\nThis is because we will continue to use this test outside of the normal distribution."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#significant-regression-line-1",
    "href": "files/lectures/W1_L1_review.html#significant-regression-line-1",
    "title": "Review of Linear Regression",
    "section": "Significant Regression Line",
    "text": "Significant Regression Line\n\nFor any model constructed, we can compute deviance.\n\nDeviance measures the unexplained variability in the outcome under the fitted model.\nLower deviance means the model fits the data better.\n\nThe actual value is not meaningful.\n\n\nTo determine the test statistic, we examine the difference between two models:\n\nFull model: includes all predictors.\nReduced model: includes only the intercept (no predictors).\n\nThen, we look at the difference between the deviances of the full and reduced models.\n\nThis difference has an approximate \\chi^2 distribution."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#significant-regression-line-2",
    "href": "files/lectures/W1_L1_review.html#significant-regression-line-2",
    "title": "Review of Linear Regression",
    "section": "Significant Regression Line",
    "text": "Significant Regression Line\n\nHypotheses\n\nH_0: \\ \\beta_1 = ... = \\beta_k = 0\nH_1: at least one \\beta_i \\ne 0\n\nTest Statistic and p-Value\n\n\\chi^2_0 = [\\text{value from R}], p = [\\text{value from R}]\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; \\alpha = [\\text{assumed } \\alpha].\n\nConclusion/Interpretation\n\n[Reject (if p &lt; \\alpha) or fail to reject (if p \\ge \\alpha)] H_0. There [is (if reject) or is not (if FTR)] sufficient evidence to suggest that at least one slope is non-zero."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#significant-regression-line-r",
    "href": "files/lectures/W1_L1_review.html#significant-regression-line-r",
    "title": "Review of Linear Regression",
    "section": "Significant Regression Line (R)",
    "text": "Significant Regression Line (R)\n\nWe will compare models to determine the significance of the line.\n\nFull model: includes all predictors.\nReduced model: includes only the intercept (no predictors).\n\nIn R,\n\n\nfull &lt;- glm(y ~ x_1 + x_2 + ... + x_k, data = dataset_name, family = \"gaussian\")\nreduced &lt;- glm(y ~ 1, data = dataset_name, family = \"gaussian\")\nanova(reduced, full, test = \"LRT\")"
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#significant-regression-line-3",
    "href": "files/lectures/W1_L1_review.html#significant-regression-line-3",
    "title": "Review of Linear Regression",
    "section": "Significant Regression Line",
    "text": "Significant Regression Line\n\nIn our example,\n\n\nfull &lt;- glm(clubhouse_happiness ~ time_with_friends + goofy_laughs + donald_grumbles,\n            data = clubhouse,\n            family = \"gaussian\")\nreduced &lt;- glm(clubhouse_happiness ~ 1, \n               data = clubhouse, \n               family = \"gaussian\")\nanova(reduced, full, test = \"LRT\")"
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#significant-regression-line-4",
    "href": "files/lectures/W1_L1_review.html#significant-regression-line-4",
    "title": "Review of Linear Regression",
    "section": "Significant Regression Line",
    "text": "Significant Regression Line\n\nHypotheses\n\nH_0: \\ \\beta_{\\text{time}} = \\beta_{\\text{goofy}} = \\beta_{\\text{Donald}} = 0\nH_1: at least one \\beta_i \\ne 0\n\nTest Statistic and p-Value\n\n\\chi^2_0 = 17305, p &lt; 0.001\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; \\alpha = 0.05.\n\nConclusion/Interpretation\n\nReject H_0. There is sufficient evidence to suggest that at least one slope is non-zero."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#significant-predictors-of-y",
    "href": "files/lectures/W1_L1_review.html#significant-predictors-of-y",
    "title": "Review of Linear Regression",
    "section": "Significant Predictors of y",
    "text": "Significant Predictors of y\n\nHypotheses\n\nH_0: \\ \\beta_i  = 0\nH_1: \\ \\beta_i \\ne 0\n\nTest Statistic and p-Value\n\nt_0 = [\\text{value from R}], p = [\\text{value from R}]\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; ; \\alpha = [\\text{assumed } \\alpha].\n\nConclusion/Interpretation\n\n[Reject (if p &lt; \\alpha) or fail to reject (if p \\ge \\alpha)] H_0. There [is (if reject) or is not (if FTR)] sufficient evidence to suggest that [predictor name] significantly predicts [outcome], after adjusting for the other predictors in the model."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#significant-predictors-of-y-r",
    "href": "files/lectures/W1_L1_review.html#significant-predictors-of-y-r",
    "title": "Review of Linear Regression",
    "section": "Significant Predictors of y (R)",
    "text": "Significant Predictors of y (R)\n\nBecause we currently are only dealing with continuous predictors, we will use the results from tidy().\n\n\n\nm %&gt;% tidy()"
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#significant-predictors-of-y-1",
    "href": "files/lectures/W1_L1_review.html#significant-predictors-of-y-1",
    "title": "Review of Linear Regression",
    "section": "Significant Predictors of y",
    "text": "Significant Predictors of y\n\nIn our example,\n\n\nm %&gt;% tidy()"
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#significant-predictors-of-y-2",
    "href": "files/lectures/W1_L1_review.html#significant-predictors-of-y-2",
    "title": "Review of Linear Regression",
    "section": "Significant Predictors of y",
    "text": "Significant Predictors of y\n\nHypotheses\n\nH_0: \\ \\beta_{\\text{time}}  = 0\nH_1: \\ \\beta_{\\text{time}} \\ne 0\n\nTest Statistic and p-Value\n\nt_0 = 5.80, p &lt; 0.001\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; ; \\alpha = 0.05.\n\nConclusion/Interpretation\n\nReject H_0. There is sufficient evidence to suggest that time with friends significantly predicts the clubhouse happiness, after adjusting for the other predictors in the model."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#significant-predictors-of-y-3",
    "href": "files/lectures/W1_L1_review.html#significant-predictors-of-y-3",
    "title": "Review of Linear Regression",
    "section": "Significant Predictors of y",
    "text": "Significant Predictors of y\n\nHypotheses\n\nH_0: \\ \\beta_{\\text{goofy}}  = 0\nH_1: \\ \\beta_{\\text{goofy}} \\ne 0\n\nTest Statistic and p-Value\n\nt_0 = 9.55, p &lt; 0.001\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; ; \\alpha = 0.05.\n\nConclusion/Interpretation\n\nReject H_0. There is sufficient evidence to suggest that goofy laughs significantly predict the clubhouse happiness, after adjusting for the other predictors in the model."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#significant-predictors-of-y-4",
    "href": "files/lectures/W1_L1_review.html#significant-predictors-of-y-4",
    "title": "Review of Linear Regression",
    "section": "Significant Predictors of y",
    "text": "Significant Predictors of y\n\nHypotheses\n\nH_0: \\ \\beta_{\\text{Donald}}  = 0\nH_1: \\ \\beta_{\\text{Donald}} \\ne 0\n\nTest Statistic and p-Value\n\nt_0 = -7.66, p &lt; 0.001\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; ; \\alpha = 0.05.\n\nConclusion/Interpretation\n\nReject H_0. There is sufficient evidence to suggest that Donald’s grumbles significantly predict the clubhouse happiness, after adjusting for the other predictors in the model."
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#reporting-results",
    "href": "files/lectures/W1_L1_review.html#reporting-results",
    "title": "Review of Linear Regression",
    "section": "Reporting Results",
    "text": "Reporting Results\n\nI typically use a table to provide results to collaborators.\n\n\n\n\nPredictor\n\\hat{\\beta}_i (95% CI for \\beta_i)\np-value\n\n\n\n\ntime with friends\n3.58 (2.37, 4.78)\n&lt; 0.001\n\n\ngoofy laughs\n0.65 (0.52, 0.80)\n&lt; 0.001\n\n\nDonald grumbles\n-1.06 (-1.34, -0.79)\n&lt; 0.001\n\n\n\n\nAs time with friends increases by 1 minute, clubhouse happiness increases by 3.58 points.\nAs goofy laughs increase by 1, clubhouse happiness increases by 0.65 points.\nAs Donald grumbles increase by 1, clubhouse happiness decreases by 1.06 points"
  },
  {
    "objectID": "files/lectures/W1_L1_review.html#wrap-up",
    "href": "files/lectures/W1_L1_review.html#wrap-up",
    "title": "Review of Linear Regression",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nThis lecture reviewed the key components of linear regression:\n\nConstructing the model in R\nInterpreting slopes and intercepts\nConstructing confidence intervals for slopes\nTesting for a significant regression line\nTesting for significant predictors\n\nWe will continue reminding ourselves of these concepts as we move into other types of regression models.\nIn the next lecture, we will review how to visualize our models."
  },
  {
    "objectID": "files/assignments/keys/A1_KEY.html",
    "href": "files/assignments/keys/A1_KEY.html",
    "title": "Assignment 1",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ssstats)\n\nCorporate is hosting a “Dunder Mifflin Sales Summit,” and Michael Scott insists Scranton should look elite. HQ collects a one-time snapshot of performance and workstyle metrics for a large group of sales employees across multiple branches (Scranton, Stamford, Utica, Albany, etc.). Each row is one employee, measured during the same month (i.e., this is independent data). In the week1.csv data file, you will find the following variables:\n\nannual_sales_usd: annual sales for the employee (USD), annualized from consistent internal accounting rules,\nclient_calls_week: average number of client calls per week,\nmeetings_week: average number of client meetings per week,\nemails_week: average number of client-facing emails per week,\navg_call_minutes: average call length (minutes),\nlead_quality: portfolio lead quality score (0–100),\ncrm_minutes_day: average minutes/day spent in CRM,\ncoffee_cups_day: average cups of coffee per day,\ndistractions_minutes_day: average minutes/day lost to distractions (pranks, “meetings,” etc.),\nmorale_score: morale score (0–10) from anonymous internal survey, and\nprinter_jams_week: average printer jams per week (yes, they tracked it).\n\nData Management\nDownload the data from Canvas and import it to R.\n\nsimulate_dunder_mifflin_snapshot &lt;- function(n = 420, seed = 2026) {\n  set.seed(seed)\n\n  # 10 continuous predictors (employee-level summaries)\n  client_calls_week        &lt;- pmax(0, rnorm(n, mean = 38, sd = 9))\n  meetings_week            &lt;- pmax(0, rnorm(n, mean = 2.4, sd = 1.0))\n  emails_week              &lt;- pmax(0, rnorm(n, mean = 22, sd = 7))\n  avg_call_minutes         &lt;- pmax(1, rnorm(n, mean = 6.2, sd = 1.6))\n  lead_quality             &lt;- pmin(100, pmax(0, rnorm(n, mean = 60, sd = 13)))\n  crm_minutes_day          &lt;- pmax(0, rnorm(n, mean = 46, sd = 16))\n  coffee_cups_day          &lt;- pmax(0, rnorm(n, mean = 2.7, sd = 1.0))\n  distractions_minutes_day &lt;- pmax(0, rnorm(n, mean = 35, sd = 18))\n  morale_score             &lt;- pmin(10, pmax(0, rnorm(n, mean = 6.9, sd = 1.4)))\n  printer_jams_week        &lt;- pmax(0, rnorm(n, mean = 1.6, sd = 1.1))\n\n  # Linear predictor (choose coefficients for reasonable scale)\n  mu &lt;- 52000 +\n    820  * client_calls_week +\n    6200 * meetings_week +\n    410  * emails_week +\n    1900 * avg_call_minutes +\n    520  * lead_quality +\n    95   * crm_minutes_day +\n    1400 * coffee_cups_day -\n    260  * distractions_minutes_day +\n    3400 * morale_score -\n    2100 * printer_jams_week\n\n  # Normally distributed outcome with noise\n  annual_sales_usd &lt;- rnorm(n, mean = mu, sd = 18000)\n\n  data.frame(\n    annual_sales_usd,\n    client_calls_week,\n    meetings_week,\n    emails_week,\n    avg_call_minutes,\n    lead_quality,\n    crm_minutes_day,\n    coffee_cups_day,\n    distractions_minutes_day,\n    morale_score,\n    printer_jams_week\n  )\n}\n\n# Create dataset\ndunder_sales &lt;- simulate_dunder_mifflin_snapshot(n = 420, seed = 2026)\n\nModel 1 (m1)\nFit a multiple linear regression model predicting annual sales (annual_sales_usd) as a function of the number of client calls per week (client_calls_week), the number of client meetings per week (meetings_week), and the number of client-facing emails per week (emails_week). Call this model m1.\n\nm1 &lt;- glm(annual_sales_usd ~ client_calls_week + meetings_week + emails_week, \n          family = \"gaussian\",\n          data = dunder_sales)\ncoefficients(m1)\n\n      (Intercept) client_calls_week     meetings_week       emails_week \n      107122.8257          971.0377         5967.7057          515.4828 \n\n\n\n1a. Based on your output, state the resulting model.\n\n\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + ... \\beta_k x_k\n\n1b. Is this a significant regression line? You must provide statistical justification for your answer.\n\nfull &lt;- glm(annual_sales_usd ~ client_calls_week + meetings_week + emails_week, \n          family = \"gaussian\",\n          data = dunder_sales)\nreduced &lt;- glm(annual_sales_usd ~ 1, \n          family = \"gaussian\",\n          data = dunder_sales)\nanova(reduced, full)\n\n\n  \n\n\n\n\n1c. Fill in the following table:\n\n\n\n\n\n\n\n\nPredictor\n\\hat{\\beta}_i (95% CI for \\beta_i)\np-value\n\n\n\n\nclient calls\n\n\n\n\nclient meetings\n\n\n\n\nclient emails\n\n\n\n\n\n\ntidy(m1, conf.int = TRUE)\n\nError in `tidy()`:\n! could not find function \"tidy\"\n\n\n1d. Which, if any, are significant predictors of annual sales? You must provide statistical justification for your answers.\n\nModel 2 (m2)\nFit a multiple linear regression model predicting annual sales (annual_sales_usd) as a function of the number of client calls per week (client_calls_week), the number of client meetings per week (meetings_week), the number of client-facing emails per week (emails_week), the number of cups of coffee per day (coffee_cups_day), and the time lost to distractions per day (distractions_minutes_day). Call this model m2.\n\nm2 &lt;- glm(annual_sales_usd ~ client_calls_week + meetings_week + emails_week + coffee_cups_day + distractions_minutes_day, \n          family = \"gaussian\",\n          data = dunder_sales)\ncoefficients(m2)\n\n             (Intercept)        client_calls_week            meetings_week \n             114801.0406                 942.9700                5995.2575 \n             emails_week          coffee_cups_day distractions_minutes_day \n                523.5902                 834.9902                -272.8283 \n\n\n\n2a. Based on your output, state the resulting model.\n\n\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + ... \\beta_k x_k\n\n2b. Is this a significant regression line? You must provide statistical justification for your answer.\n\nfull &lt;- glm(annual_sales_usd ~ client_calls_week + meetings_week + emails_week + coffee_cups_day + distractions_minutes_day, \n          family = \"gaussian\",\n          data = dunder_sales)\nreduced &lt;- glm(annual_sales_usd ~ 1, \n          family = \"gaussian\",\n          data = dunder_sales)\nanova(reduced, full)\n\n\n  \n\n\n\n\n2c. Fill in the following table:\n\n\n\n\n\n\n\n\nPredictor\n\\hat{\\beta}_i (95% CI for \\beta_i)\np-value\n\n\n\n\nclient calls\n\n\n\n\nclient meetings\n\n\n\n\nclient emails\n\n\n\n\ncoffee cups\n\n\n\n\ndistractions\n\n\n\n\n\n\ntidy(m1, conf.int = TRUE)\n\nError in `tidy()`:\n! could not find function \"tidy\"\n\n\n2d. Which, if any, are significant predictors of annual sales? You must provide statistical justification for your answers.\nComparing the Models\n3a. Let’s compare the two models side-by-side. Fill in the following table:\n\n\n\n\n\n\n\n\n\n\n\nPredictor\nm1 \\hat{\\beta}_i\nm1 p-value\nm2 \\hat{\\beta}_i\nm2 p-value\n\n\n\n\nclient calls\n\n\n\n\n\n\nclient meetings\n\n\n\n\n\n\nclient emails\n\n\n\n\n\n\ncoffee cups\nxxxxxxx\nxxxx\n\n\n\n\ndistractions\nxxxxxxx\nxxxx\n\n\n\n\n\n3b. How different are the model results? You should compare the coefficients and significance of each predictor across the two models.\nModel Selection\n4a. Find the R_{\\text{adj}}^2 for each model. What do these values mean?\n\nm1 %&gt;% r_squared()\n\n[1] 0.2104\n\nm2 %&gt;% r_squared()\n\n[1] 0.2508\n\n\n\nm1:\nm2:\n\n\n4b. Based on R_{\\text{adj}}^2, which model is preferred? Why?\n\n4c. Find the AIC and BIC values for each model.\n\nAIC(m1)\n\n[1] 9542.922\n\nBIC(m1)\n\n[1] 9563.123\n\nAIC(m2)\n\n[1] 9522.795\n\nBIC(m2)\n\n[1] 9551.077\n\n\n\n\n\nModel\nAIC\nBIC\n\n\n\n\nm1\n\n\n\n\nm2\n\n\n\n\n\n4d. Based on AIC and BIC, which model is preferred? Why?\n\n4e. Considering your answers to 3b and 3d, which model will you choose as your final model? Why?\n\nReporting the Final Model\n5a. Using the model chosen in 4e, paste in either the appropriate model summary table (from 1c or 2c).\n\n5b. Provide formal interpretations for each predictor in the final model. Note that all predictors are shown for completeness; please remove any predictors that do not appear in your final model.\n\nclient calls: \nclient meetings: \nclient emails: \ncoffee cups: \ndistractions: \n\n5c. Construct an appropriate data visualization to help explain the relationships in your final model. While there is no single “right” answer here, you should consider your audience (…potentially Michael Scott…) and what would be most helpful for them to understand what the math is suggesting. For uniformity (for grading purposes), let’s have annual sales (annual_sales_usd) on the y-axis and number of client meetings (meetings_week) on the x-axis. You may include additional aesthetics (color, size, shape, facets, etc.) as you see fit. For predicted values, please plug in the median value for all other predictors in the model.\n5d. Pretend that you are preparing to meet with Michael Scott to review the analysis results. Summarize the key takeaways from your model and visualization? Remember your audience here – Michael is not a statistician, so you should avoid jargon and technical language. What would be most helpful for him to know to improve sales performance at Dunder Mifflin?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Data Science II",
    "section": "",
    "text": "Welcome to Statistics for Data Science II!\nThis is an 8 week asynchronous course that builds on the foundations of statistics and data analysis. We will focus on the application of statistical models. In particular we will focus on the following regression strategies:\n\nLinear regression (normal distribution)\nGamma regression\nBeta regression\nBinary logistic regression\nOrdinal logistic regression\nMultinomial logistic regression\nPoisson and negative binomial regressions\n\nEach week will consist of multiple lecture videos, a review of that week’s programming, a project to demonstrate understanding, and a formal quiz to assess comprehension.\nWhile there are no live sessions, students will engage with their peers and instructor through Discord. Students are expected to ask questions, answer questions (when able), and share resources."
  },
  {
    "objectID": "files/assignments/A2.html",
    "href": "files/assignments/A2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "After the success of Michael’s “Sales Summit Scorecard,” corporate decides to expand the analysis to understand structural differences across employees. This time, they collect a one-time snapshot of sales employees across multiple Dunder Mifflin branches, focusing on how role structure and branch culture relate to sales performance.\nEach row in the dataset represents one employee. All variables summarize employee behavior and performance over the same time period. In the week2.csv data file, you will find the following variables:\n\nannual_sales_usd: annual sales for the employee (USD),\nsale_role: sales role (Inside vs. Outside),\nbranch: branch location (Scranton, Stamford, Utica, Albany),\nclient_calls_week: average number of client calls per week,\nmeetings_week: average number of client meetings per week,\nemails_week: average number of client-facing emails per week,\navg_call_minutes: average call length (minutes),\nlead_quality: portfolio lead quality score (0–100),\ncrm_minutes_day: average minutes/day spent in CRM,\ncoffee_cups_day: average cups of coffee per day,\ndistractions_minutes_day: average minutes/day lost to distractions (pranks, “meetings,” etc.), and\nmorale_score: morale score (0–10) from anonymous internal survey.\n\nData Management\n0a. Download the data from Canvas and import it to R.\n0b. Examine what is contained in the sales_role variable by producing a frequency table.\n0c. Examine what is contained in the branch variable by producing a frequency table.\n0d. Find summary statistics (mean and sd) for the continuous predictor variables in the dataset. Split this by branch.\nConstructing the Model\n1a. Fit a multiple linear regression model predicting annual sales (annual_sales_usd) as a function of the number of client calls per week (client_calls_week), the number of client meetings per week (meetings_week), and the branch (branch; use Scranton as the reference group). Call this model m1.\n1b. Based on your output, state the resulting model.\n\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + ... \\beta_k x_k\n\n1c. Is this a significant regression line? You must provide statistical justification for your answer. Assume \\alpha=0.05.\n1d. Fill in the following table:\n\n\n\n\n\n\n\n\nPredictor\n\\hat{\\beta}_i (95% CI for \\beta_i)\np-value\n\n\n\n\nclient calls\n\n\n\n\nclient meetings\n\n\n\n\nbranch (Stamford)\n\n\n\n\nbranch (Utica)\n\n\n\n\nbranch (Albany)\n\n\n\n\n\n1e. Which, if any, are significant predictors of annual sales? You must provide statistical justification for your answers. Assume \\alpha=0.05.\nModel Assumptions\n2a. State the assumptions on the linear model.\n2b. Construct the appropriate graph(s) to check assumptions listed in 2a.\n2c. Do we meet the assumptions for the linear model? Are we okay to proceed with statistical inference?\nModel Interpretation\n3a. Provide formal interpretations for each predictor in the model.\n\nclient calls: \nclient meetings: \nbranch (Stamford): \nbranch (Utica): \nbranch (Albany): \n\n3b. Find the adjusted R^2 for the model. What does this mean in context?\n3c. Construct an appropriate data visualization to help explain the relationships in your final model.For uniformity (for grading purposes), let’s again have annual sales (annual_sales_usd) on the y-axis, the number of client meetings (meetings_week) on the x-axis, and the lines defined by the branch (branch). For predicted values, please plug plausible values for the other predictors in the model.\n3d. Pretend that you are preparing to meet with Michael Scott to review the analysis results. Summarize the key takeaways from your model and visualization. Remember your audience here – Michael is not a statistician, so you should avoid jargon and technical language. What would be most helpful for him to know to improve sales performance at Dunder Mifflin? What has changed from last week’s analysis?"
  },
  {
    "objectID": "files/assignments/A1.html",
    "href": "files/assignments/A1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ssstats)\n\nCorporate is hosting a “Dunder Mifflin Sales Summit,” and Michael Scott insists Scranton should look elite. HQ collects a one-time snapshot of performance and workstyle metrics for a large group of sales employees across multiple branches (Scranton, Stamford, Utica, Albany, etc.). Each row is one employee, measured during the same month (i.e., this is independent data). In the week1.csv data file, you will find the following variables:\n\nannual_sales_usd: annual sales for the employee (USD), annualized from consistent internal accounting rules,\nclient_calls_week: average number of client calls per week,\nmeetings_week: average number of client meetings per week,\nemails_week: average number of client-facing emails per week,\navg_call_minutes: average call length (minutes),\nlead_quality: portfolio lead quality score (0–100),\ncrm_minutes_day: average minutes/day spent in CRM,\ncoffee_cups_day: average cups of coffee per day,\ndistractions_minutes_day: average minutes/day lost to distractions (pranks, “meetings,” etc.),\nmorale_score: morale score (0–10) from anonymous internal survey, and\nprinter_jams_week: average printer jams per week (yes, they tracked it).\n\nData Management\nDownload the data from Canvas and import it to R.\nModel 1 (m1)\nFit a multiple linear regression model predicting annual sales (annual_sales_usd) as a function of the number of client calls per week (client_calls_week), the number of client meetings per week (meetings_week), and the number of client-facing emails per week (emails_week). Call this model m1.\n\n1a. Based on your output, state the resulting model.\n\n\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + ... \\beta_k x_k\n\n1b. Is this a significant regression line? You must provide statistical justification for your answer.\n\n1c. Fill in the following table:\n\n\n\n\n\n\n\n\nPredictor\n\\hat{\\beta}_i (95% CI for \\beta_i)\np-value\n\n\n\n\nclient calls\n\n\n\n\nclient meetings\n\n\n\n\nclient emails\n\n\n\n\n\n1d. Which, if any, are significant predictors of annual sales? You must provide statistical justification for your answers.\n\nModel 2 (m2)\nFit a multiple linear regression model predicting annual sales (annual_sales_usd) as a function of the number of client calls per week (client_calls_week), the number of client meetings per week (meetings_week), the number of client-facing emails per week (emails_week), the number of cups of coffee per day (coffee_cups_day), and the time lost to distractions per day (distractions_minutes_day). Call this model m2.\n\n2a. Based on your output, state the resulting model.\n\n\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + ... \\beta_k x_k\n\n2b. Is this a significant regression line? You must provide statistical justification for your answer. Assume \\alpha=0.05.\n\n2c. Fill in the following table:\n\n\n\n\n\n\n\n\nPredictor\n\\hat{\\beta}_i (95% CI for \\beta_i)\np-value\n\n\n\n\nclient calls\n\n\n\n\nclient meetings\n\n\n\n\nclient emails\n\n\n\n\ncoffee cups\n\n\n\n\ndistractions\n\n\n\n\n\n2d. Which, if any, are significant predictors of annual sales? You must provide statistical justification for your answers. Assume \\alpha=0.05.\n\nComparing the Models\n3a. Let’s compare the two models side-by-side. Fill in the following table:\n\n\n\n\n\n\n\n\n\n\n\nPredictor\nm1 \\hat{\\beta}_i\nm1 p-value\nm2 \\hat{\\beta}_i\nm2 p-value\n\n\n\n\nclient calls\n\n\n\n\n\n\nclient meetings\n\n\n\n\n\n\nclient emails\n\n\n\n\n\n\ncoffee cups\nxxxxxxx\nxxxx\n\n\n\n\ndistractions\nxxxxxxx\nxxxx\n\n\n\n\n\n3b. How different are the model results? You should compare the coefficients and significance of each predictor across the two models.\n\nModel Selection\n4a. Find the R_{\\text{adj}}^2 for each model. What do these values mean?\n\nm1:\nm2:\n\n\n4b. Based on R_{\\text{adj}}^2, which model is preferred? Why?\n\n4c. Find the AIC and BIC values for each model.\n\n\n\nModel\nAIC\nBIC\n\n\n\n\nm1\n\n\n\n\nm2\n\n\n\n\n\n4d. Based on AIC and BIC, which model is preferred? Why?\n\n4e. Considering your answers to 4b and 4d, which model will you choose as your final model? Why?\n\nReporting the Final Model\n5a. Using the model chosen in 4e, paste in either the appropriate model summary table (from 1c or 2c).\n\n5b. Provide formal interpretations for each predictor in the final model. Note that all predictors are shown for completeness; please remove any predictors that do not appear in your final model.\n\nclient calls: \nclient meetings: \nclient emails: \ncoffee cups: \ndistractions: \n\n5c. Construct an appropriate data visualization to help explain the relationships in your final model. While there is no single “right” answer here, you should consider your audience (…potentially Michael Scott…) and what would be most helpful for them to understand what the math is suggesting. For uniformity (for grading purposes), let’s have annual sales (annual_sales_usd) on the y-axis and number of client meetings (meetings_week) on the x-axis. For predicted values, please plug in the median() value for all other predictors in the model.\n5d. Pretend that you are preparing to meet with Michael Scott to review the analysis results. Summarize the key takeaways from your model and visualization. Remember your audience here – Michael is not a statistician, so you should avoid jargon and technical language. What would be most helpful for him to know to improve sales performance at Dunder Mifflin?"
  },
  {
    "objectID": "files/syllabus.html",
    "href": "files/syllabus.html",
    "title": "Abbreviated Syllabus",
    "section": "",
    "text": "Note: this is an abbreviated syllabus. The full syllabus is available through Classmate and Canvas.\n\nInstructor Information\n\nDr. Samantha Seals\nOffice: Dr. Seals is on sabbatical for Spring 2026 and will not be on campus regularly.\n\n\n\nOffice Hours\n\nBy appointment only. Please reach out to Dr. Seals directly to schedule a meeting.\n\n\n\nGrading and Evaluation\nThe course grade will be determined as follows:\n\nPractice and Participation (10%): Students earn credit by engaging with course material and their classmates through the class Discord. Half of these points come from Discord participation, which includes both asking and answering questions. As a formal deliverable, students will complete a weekly check-in quiz by Sunday night at 11:59 pm Central. No extensions will be granted nor will make up submissions be accepted.\nWeekly Projects (30%): All projects will be completed individually using R, formatted with Quarto. Projects are due on Tuesdays by 11:59 pm Central. Note: do not wait until Sunday before the check in to work on your projects. After the first round of grading, projects will be reopened for updates; students will have two weeks from that point to submit revisions for (full) credit back.\nQuizzes (20%): There will be a quiz at the end of each week; these quizzes open Tuesday at 12:00 am Central and are due Tuesdays at 11:59 pm Central. These quizzes will focus on concepts from that week’s lectures and applications. Quizzes are open book/note, but collaboration is not allowed. No extensions will be granted nor will make up quizzes be available.\nProject 7 (10%): This project will be a cumulative project that incorporates material from the entire course. It will be no different from other projects completed other than it combines all modeling approaches into one project. This project is exempt from the revision policy and is due on Tuesday, February 24, 2026 by 11:59 pm Central.\nFinal Exam (30%): The final exam will be a timed concepts-based exam. While there may be some calculations needed, you will not be processing raw data. There will be no programming expectations on the final exam. The timed final exam will be on Friday, February 27, 2026 The date of this exam will not change, so please make arrangements to be free for a continuous 3 hour block. Once the exam is open, the timer for 3 hours starts and continues, even if you close your browser window. No extensions will be granted nor will make up final exams be accepted."
  },
  {
    "objectID": "files/lectures/W2_L3a_why_type_3.html#introduction",
    "href": "files/lectures/W2_L3a_why_type_3.html#introduction",
    "title": "‘Types’ of Sums of Squares",
    "section": "Introduction",
    "text": "Introduction\n\nYou may have noticed that when we are using car::Anova() we are asked to specify a type argument."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#introduction",
    "href": "files/lectures/W1_L4_diagnostics.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\n\nRecall the glm,\n\n\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... \\beta_k x_k + \\varepsilon\n\n\nIn this model, \\varepsilon is the residual error term.\nRecall that the residual error (how far away the observation is from the response surface) is defined as \\varepsilon = y - \\hat{y}\nNote that\n\ny is the observed value\n\\hat{y} is the predicted value"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\nIn addition to checking assumptions, we should also perform model diagnostics to identify potential issues with our model.\nCommon diagnostics include:\n\nAssessing overall model fit (e.g., R^2, AIC, BIC).\nIdentifying outliers (points that are far from the predicted values).\nIdentifying influential observations (points that have a large impact on the model’s estimates).\nChecking for multicollinearity (high correlation between predictors)."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-1",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-1",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\nThe diagnostics needed depend on the context of our analysis and the specific model we are using.\n\nContext matters! Applied statistics: we do not want our model to be perfect, we want it to be generalizable. Does the interpretation make intuitive (scientific) sense? If not, why? Is there a weird data point (or points) affecting our analysis?\nContext matters! Machine learning/predictive modeling: we care about predictive accuracy, not interpretability. Does the model predict well on new data? If not, why? Is there overfitting or underfitting? Is that due to weird data points?"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit",
    "text": "Model Diagnostics: Model Fit\n\nWe can assess overall model fit using metrics such as:\n\nR^2: proportion of variance in the response variable explained by the model.\nAdjusted R^2: adjusts R^2 for the number of predictors in the model.\nAIC (Akaike Information Criterion): measures the relative quality of the specified model for the given data. This balances goodness-of-fit and model complexity.\nBIC (Bayesian Information Criterion): same conecpt of AIC, but has a stronger penalty as the number of parameters (p) estimated increases.\n\nIn our course, we are only estimating \\beta_i, which we typically say that i = 1, ..., k predictors + 1 intercept. Thus, p = k + 1.\nIf we are accounting for correlated data (not in this course), we are introducing additional parameters to estimate (e.g., correlation structure parameters)."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-r2",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-r2",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit – R^2",
    "text": "Model Diagnostics: Model Fit – R^2\n\nIf we want to know how well the model fits this particular dataset, we can look at the R^2.\n\nR^2 is the proportion of variance explained by the model.\n\n\nR^2 = \\frac{\\text{SSReg}}{\\text{SSTot}}\n\nBecause it is a proportion, R^2 \\in [0, 1] and is independent of the units of y.\nIf R^2 \\to 0, the model does not fit the data well.\nIf R^2 \\to 1, the model fits the data well.\n\nNote that if R^2=1, all observations fall on the response surface."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-r2-1",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-r2-1",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit – R^2",
    "text": "Model Diagnostics: Model Fit – R^2\n\nRemember that we are partitioning the variability in y (SSTot), which is constant, into two pieces:\n\nThe variability explained by the regression model (SSReg).\nThe variability explained by outside sources (SSE).\n\nAs predictors are added to the model, we necessarily increase SSReg / decrease SSE.\n\nRemember, SSTot = SSReg + SSE.\n\nWe want a measure of model fit that is resistant to this fluctuation,\n\nR^2_{\\text{adj}} = 1 - \\left( \\frac{n-1}{n-k-1} \\right) \\left( 1 - R^2 \\right),\n\nRemember, k is the number of predictor terms in the model."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-r2-r",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-r2-r",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit – R^2 (R)",
    "text": "Model Diagnostics: Model Fit – R^2 (R)\n\nWe will use the ssstats package to check diagnostics.\nFor normality, we can use the r_squared() function.\n\n\nmodel %&gt;% r_squared()"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-r2-2",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-r2-2",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit – R^2",
    "text": "Model Diagnostics: Model Fit – R^2\n\nRecall our example model, m1:\n\n\n\\hat{y} = 3 + 1.5 x + \\hat{\\varepsilon}\n\n\nm1 %&gt;% r_squared()\n\n[1] 0.851\n\n\n\nRecall our example model, m6:\n\n\n\\hat{y} = 2 + 1.1 x_1 - 0.8 x_2 + \\hat{\\varepsilon}\n\n\nm6 %&gt;% r_squared()\n\n[1] 0.0272"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-r2-3",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-r2-3",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit – R^2",
    "text": "Model Diagnostics: Model Fit – R^2\n\nWhat do I do when there is a very low R^2 value? The questions I ask:\n\nWhat did the assumption graphs look like?\n\nDo we actually meet the normality assumption?\n-If not, should we consider taking a different methodological approach?\n\nI should graph the data to see what it looks like.\n\nIs the relationship non-linear?\nIs there an outlier affecting the relationship?\nIs the data just super noisy?\n\nIs there a variable missing from the model that could help explain the response?"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-r2-4",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-r2-4",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit – R^2",
    "text": "Model Diagnostics: Model Fit – R^2\n\nRemember that the R^2 value is not the end-all, be-all.\nLike everything in statistics/data science, context matters.\n\nIn some applications, low R^2 values are common.\n\ne.g., human behavior, social sciences, biological systems.\n\nIn other applications, high R^2 values are expected.\n\ne.g., physical sciences, engineering.\n\n\nAlways interpret R^2 in the context of your specific field and research question."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit – AIC and BIC",
    "text": "Model Diagnostics: Model Fit – AIC and BIC\n\nBecause the calculation of the AIC and BIC depend on the likelihood, we will not dive into the formulas here.\nInstead, let’s discuss how the AIC and BIC penalize our models.\n\nBoth AIC and BIC balance model fit and model complexity.\nAIC imposes a penalty of 2p.\nBIC imposes a penalty of p \\times \\log(n).\n\nRemember, p is the number of parameters estimated in the model.\n\nIn this course, we will not see a great difference between the AIC and BIC values.\nHowever, in more complex models (e.g., analyzing dependent data, where p &gt; k), the difference can be more pronounced."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic-1",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic-1",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit – AIC and BIC",
    "text": "Model Diagnostics: Model Fit – AIC and BIC\n\nWhat do the values actually mean?\n\nInherently, we cannot “interpret” the values – they are arbitrary numbers based on the data itself and the model specified.\n\nHow do we use these?\n\nWe use these to compare models.\nThe model with the lowest AIC or BIC is fits the data “better”.\n\nConsider an example dataset with y, x_1, x_2, and x_3; we fit the following models:\n\nm1: y \\sim x_1 (AIC = 250, BIC = 255)\nm2: y \\sim x_1 + x_2 (AIC = 240, BIC = 248)\nm3: y \\sim x_1 + x_2 + x_3 (AIC = 245, BIC = 256)\n\nWe would then declare m2 the best fitting model as its AIC and BIC values are the lowest."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic-r",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic-r",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit – AIC and BIC (R)",
    "text": "Model Diagnostics: Model Fit – AIC and BIC (R)\n\nTo check AIC and BIC, we will use the standard functions in base R,\n\n\nmodel %&gt;% AIC()\nmodel %&gt;% BIC()"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#example-set-up",
    "href": "files/lectures/W1_L4_diagnostics.html#example-set-up",
    "title": "Model Diagnostics",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nLet’s return to our Mickey and friends example,\n\n\nclubhouse &lt;- read_csv(\"https://raw.githubusercontent.com/samanthaseals/SDSII/refs/heads/main/files/data/lectures/W1_mickey_clubhouse.csv\")\nhead(clubhouse)"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic-2",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic-2",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit – AIC and BIC",
    "text": "Model Diagnostics: Model Fit – AIC and BIC\n\nRecall one of the models we examined in the visualization lecture.\n\nWe looked at clubhouse happiness (clubhouse_happiness) as a function of time spent with friends (time_with_friends), big, goofy laughs (goofy_laughs), and how much Donald grumbles (donald_grumbles).\n\n\n\nh1 &lt;- glm(clubhouse_happiness ~ time_with_friends + goofy_laughs + donald_grumbles,\n          family = \"gaussian\",\n          data = clubhouse)\nh1 %&gt;% tidy()\n\n\n  \n\n\n\n\n\\hat{\\text{happiness}} = 47.58 + 3.58 \\text{ time} + 0.66 \\text{ laughs} - 1.06 \\text{ grumbles}"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic-3",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic-3",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit – AIC and BIC",
    "text": "Model Diagnostics: Model Fit – AIC and BIC\n\nLet’s now consider a set of models,\n\nh1: clubhouse_happiness ~ time_with_friends + goofy_laughs + donald_grumbles\nh2: clubhouse_happiness ~ time_with_friends + goofy_laughs\nh3: clubhouse_happiness ~ goofy_laughs + donald_grumbles\n\n\n\nh1 &lt;- glm(clubhouse_happiness ~ time_with_friends + goofy_laughs + donald_grumbles, family = \"gaussian\", data = clubhouse)\nh2 &lt;- glm(clubhouse_happiness ~ time_with_friends + goofy_laughs, family = \"gaussian\", data = clubhouse)\nh3 &lt;- glm(clubhouse_happiness ~ goofy_laughs + donald_grumbles, family = \"gaussian\", data = clubhouse)"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic-4",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic-4",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit – AIC and BIC",
    "text": "Model Diagnostics: Model Fit – AIC and BIC\n\nNow, let’s compare the AIC values for these models.\n\n\nAIC(h1) # time_with_friends + goofy_laughs + donald_grumbles\n\n[1] 2222.259\n\nAIC(h2) # time_with_friends + goofy_laughs\n\n[1] 2274.454\n\nAIC(h3) # goofy_laughs + donald_grumbles\n\n[1] 2252.592\n\n\n\nAccording to the AIC, the full model (time_with_friends + goofy_laughs + donald_grumbles) fits the data best."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic-5",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic-5",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit – AIC and BIC",
    "text": "Model Diagnostics: Model Fit – AIC and BIC\n\nNow, let’s compare the BIC values for these models.\n\n\nBIC(h1) # time_with_friends + goofy_laughs + donald_grumbles\n\n[1] 2240.778\n\nBIC(h2) # time_with_friends + goofy_laughs\n\n[1] 2289.269\n\nBIC(h3) # goofy_laughs + donald_grumbles\n\n[1] 2267.407\n\n\n\nAccording to the BIC, the full model (time_with_friends + goofy_laughs + donald_grumbles) fits the data best."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic-6",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-model-fit-aic-and-bic-6",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Model Fit – AIC and BIC",
    "text": "Model Diagnostics: Model Fit – AIC and BIC\n\nPutting the values in a table,\n\n\n\n\nModel\nAIC\nBIC\n\n\n\n\nh1\n2222.259\n2240.778\n\n\nh2\n2274.454\n2289.269\n\n\nh3\n2252.592\n2267.407\n\n\n\n\nBoth the AIC and BIC indicate that h1 is the best fitting model.\nWe can see that BIC &gt; AIC consistently, which is expected.\n\nRemember, the BIC has a stronger penalty for the number of terms in the model."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-outliers",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-outliers",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Outliers",
    "text": "Model Diagnostics: Outliers\n\nThe word “outlier” is part of our every day language.\nMathematically, an outlier is an observation that lies an abnormal distance from other values in a random sample from a population.\n\nWe can have outliers in y (response) or in x_i (predictors).\n\nOutliers in y are observations with large residuals and what we mean by “outliers” in the formal sense.\nOutliers can have a significant impact on the regression model.\n\nThey can skew the results, leading to biased estimates of the regression coefficients.\nThey can also affect the overall fit of the model, leading to misleading conclusions."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-outliers-1",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-outliers-1",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Outliers",
    "text": "Model Diagnostics: Outliers\n\nRecall the (mathematical) definition of the residual:\n\n\ne_i = y_i - \\hat{y}_i\n\n\nWe are instead interested in the standardized residual, which accounts for the leverage (h_i) of the observation.\n\n\ne_{i_{\\text{std}}} = \\frac{e_i}{\\sqrt{\\text{MSE}(1-h_i)}}\n\n\nUltimately, we want all of our residuals as close to 0 as possible. However,\n\nif |e_{i_{\\text{std}}}| &gt; 2.5 \\ \\to \\  outlier, and\nif |e_{i_{\\text{std}}}| &gt; 3 \\ \\to \\  extreme outlier."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-outliers-r",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-outliers-r",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Outliers (R)",
    "text": "Model Diagnostics: Outliers (R)\n\nWe will use the rstandard() function in base R to find the residuals.\nFor ease of examining in large datasets, we will use it to create a “flag.”\n\n\ndataset &lt;- dataset %&gt;%\n  mutate(outlier =  if_else(abs(rstandard(model))&gt;2.5, \"Suspected\", \"Not Suspected\"))\n\n\nThen, we can count the number of outliers,\n\n\ndataset %&gt;% count(outlier)\n\n\nor we could only look at outliers from the dataset,\n\n\nall_outliers &lt;- dataset %&gt;% filter(outlier == TRUE)\n\n\nor we also exclude outliers from the dataset,\n\n\nno_outliers &lt;- dataset %&gt;% filter(outlier == FALSE)"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-outliers-2",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-outliers-2",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Outliers",
    "text": "Model Diagnostics: Outliers\n\nLet’s return to our Mickey and friends example, recalling the models we created for the AIC and BIC example.\n\nh1: clubhouse_happiness ~ time_with_friends + goofy_laughs + donald_grumbles\nh2: clubhouse_happiness ~ time_with_friends + goofy_laughs\nh3: clubhouse_happiness ~ goofy_laughs + donald_grumbles\n\nLet’s examine the outliers. First, we create the flags.\n\n\nclubhouse &lt;- clubhouse %&gt;%\n  mutate(outlier_h1 =  if_else(abs(rstandard(h1))&gt;2.5, \"Suspected\", \"Not Suspected\"),\n         outlier_h2 =  if_else(abs(rstandard(h2))&gt;2.5, \"Suspected\", \"Not Suspected\"),\n         outlier_h3 =  if_else(abs(rstandard(h3))&gt;2.5, \"Suspected\", \"Not Suspected\"))\n\n\nNote!!! I am adding the flags for each model because the outliers can differ by model, and I want to demonstrate that here.\n\nIn my every day life, I only look at diagnostics for the final model we are using."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-outliers-3",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-outliers-3",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Outliers",
    "text": "Model Diagnostics: Outliers\n\nNow that we have the flags, let’s look at the number of outliers by model:\n\n\nclubhouse %&gt;% count(outlier_h1)\n\n\n  \n\n\nclubhouse %&gt;% count(outlier_h2)\n\n\n  \n\n\nclubhouse %&gt;% count(outlier_h3)"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-outliers-4",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-outliers-4",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Outliers",
    "text": "Model Diagnostics: Outliers\n\nLooking at the residual patterns across models:\n\n\nclubhouse %&gt;% count(outlier_h1, outlier_h2, outlier_h3)\n\n\n  \n\n\n\n\nThis demonstrates that just because it is a suspected outlier in one model, it may not be a suspected outlier in another model."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-outliers-5",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-outliers-5",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Outliers",
    "text": "Model Diagnostics: Outliers\n\nFinally, how do we handle outliers?\nMy approach:\n\nInvestigate the outlier: try to determine if it is a data entry error or a valid observation.\n\nThis requires discussion with those involved with data collection and is not possible in the classroom setting.\ne.g., BMI of 93 in the Jackson Heart Study\n\nSensitivity analysis: fit the model with and without the outlier(s) to see how much the results change.\n\nPlease note that we should always be cautious when removing outliers from the dataset.\n\nWe do not want to curate data to fit our preconceived notions.\nAlways document any decisions made regarding outliers and justify them based on nonstatistical reasoning (e.g., data entry error that cannot be remedied)."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-leverage-and-influence",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-leverage-and-influence",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Leverage and Influence",
    "text": "Model Diagnostics: Leverage and Influence\n\nNow, what about outliers in x_i (the predictors)?\nThese points can have high leverage and/or be influential.\n\nLeverage measures how far an observation’s predictor values are from the mean of the predictor values.\nInfluential points are observations that have a significant impact on the regression model’s estimates.\n\ni.e., they are observations that are “pulling” the regression line towards themselves.\n\n\nWe will use Cook’s distance to assess leverage and influence.\nLike outliers, an observation’s leverage and influence values depend on the model that was specified.\n\ni.e., we will need to check these diagnostics for each model we are considering."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-leverage-and-influence-1",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-leverage-and-influence-1",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Leverage and Influence",
    "text": "Model Diagnostics: Leverage and Influence\n\nBefore we talk about Cook’s distance, let’s consider the leverage, or h_i. Mathematically,\n\n\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}\n\n\nConceputally, leverage measures how far an observation’s predictor values are from the mean of the predictor values and compares it to the overall variability in the predictor values.\n\nObservations with high leverage have predictor values that are far from the mean of the predictor values.\n\nOn the previous slide, we said we will instead use Cook’s distance, which combines information about both the residual and leverage of each observation.\n\nThus, we will not find the h_i values directly."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-leverage-and-influence-2",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-leverage-and-influence-2",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Leverage and Influence",
    "text": "Model Diagnostics: Leverage and Influence\n\nCook’s distance combines information about the residual and leverage of each observation. Mathematically,\n\n\nD_i = \\frac{1}{(k+1)\\text{MSE}} \\left( \\frac{h_i}{(1-h_i)^2} \\right)e_i^2\n\n\nwhere\n\ne_i is the residual for observation i,\nh_i is the leverage for observation i,\nk is the number of predictor terms in the model, and\nMSE is the mean squared error of the model."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-leverage-and-influence-3",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-leverage-and-influence-3",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Leverage and Influence",
    "text": "Model Diagnostics: Leverage and Influence\n\nHow do we interpret Cook’s distance?\n\nThere are different cutoff rules suggested in the literature.\nCommon cutoff: if D_i &gt; 4/(n - k - 1), then observation i is considered influential.\n\nFor a more intuitive approach, we will graph the Cook’s distance values.\n\nI look for observations with substantially larger Cook’s distance values than the rest of the observations.\nThese are the influential points."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-influence-and-leverage-r",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-influence-and-leverage-r",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Influence and Leverage (R)",
    "text": "Model Diagnostics: Influence and Leverage (R)\n\nWe will return to the ssstats package and use the cooks() function to generate the graph we need.\n\n\nmodel %&gt;% cooks()"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-influence-and-leverage",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-influence-and-leverage",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Influence and Leverage",
    "text": "Model Diagnostics: Influence and Leverage\n\nApplying this to our example,\n\n\nh1 %&gt;% cooks()"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-influence-and-leverage-1",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-influence-and-leverage-1",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Influence and Leverage",
    "text": "Model Diagnostics: Influence and Leverage\n\nApplying this to our example,\n\n\nh2 %&gt;% cooks()"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-influence-and-leverage-2",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-influence-and-leverage-2",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Influence and Leverage",
    "text": "Model Diagnostics: Influence and Leverage\n\nApplying this to our example,\n\n\nh3 %&gt;% cooks()"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-influence-and-leverage-3",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-influence-and-leverage-3",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Influence and Leverage",
    "text": "Model Diagnostics: Influence and Leverage\n\nLet’s put these graph side-by-side for comparison."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-leverage-and-influence-4",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-leverage-and-influence-4",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Leverage and Influence",
    "text": "Model Diagnostics: Leverage and Influence\n\nFinally, how do we handle influential points? Recall: they are technically outliers….\nMy approach:\n\nInvestigate the outlier: try to determine if it is a data entry error or a valid observation.\n\nThis requires discussion with those involved with data collection and is not possible in the classroom setting.\n\nSensitivity analysis: fit the model with and without the outlier(s) to see how much the results change.\n\nAgain, we should not formally remove influential points without investigation.\n\nAlways document any decisions made regarding influential points and justify them based on nonstatistical reasoning (e.g., data entry error that cannot be remedied)."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nMulticollinearity occurs when two or more predictor variables in a regression model are highly correlated.\nConsequences of multicollinearity:\n\nInflated standard errors of the regression coefficients.\nUnstable coefficient estimates that can change significantly with small changes in the data.\nDifficulty in determining the individual effect of each predictor on the response variable.\n\nMulitcollinearity \\to predictors contain overlapping information.\n\n\nNote that multicollinearity doesn’t necessarily “break” the model, but instead, makes the coefficients hard to trust."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-1",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-1",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nTo assess multicollinearity, we can use the variance inflation factor (VIF).\n\nVIF measures how much the variance of a coefficient is inflated because predictors overlap.\n\nMathematically,\n\n\n\\text{VIF}_j = \\frac{1}{1 - R_j^2}\n\n\nhere,\n\nj indexes the predictor number; j = 1, 2, ..., k, and\nR_j^2 is the R^2 when regressing predictor j on all other predictors in the model.\n\ni.e., we find the R^2 value for j ~ x1 + … xj-1 + xj+1 + … + xk."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-2",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-2",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nHow do we make the call that multicollinearity exists?\n\nLike Cook’s distance, there are different cutoff rules suggested in the literature.\nTypical cutoff: if VIF &gt; 5 (or 10), then multicollinearity is a concern.\n\n\n\n\\text{VIF}_j = \\frac{1}{1 - R_j^2}\n\n\nThe closer VIF is to 1, the less multicollinearity there is.\n\nIf R_j^2 \\to 0 (the other k-1 predictors do a terrible job of predicting x_j) then \\text{VIF}_j \\to 1 (no multicollinearity).\nIf R_j^2 \\to 1 (the other k-1 predictors do a fantastic job of predicting x_j), then \\text{VIF}_j \\to \\infty (severe multicollinearity)."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-r",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-r",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity (R)",
    "text": "Model Diagnostics: Multicollinearity (R)\n\nWe will use the vif() function from the car package.\n\n\nmodel %&gt;% car::vif()\n\n\nNote: recall that the car package overwrites some functions from tidyverse, so I typically do not load the full library."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-3",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-3",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nLet’s check the multicollinearity in our Mickey and friends examples,\n\n\nh1 %&gt;% car::vif()\n\ntime_with_friends      goofy_laughs   donald_grumbles \n         1.017538          1.005408          1.012101 \n\nh2 %&gt;% car::vif()\n\ntime_with_friends      goofy_laughs \n         1.005401          1.005401 \n\nh3 %&gt;% car::vif()\n\n   goofy_laughs donald_grumbles \n       1.000029        1.000029"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-4",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-4",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nChip and Dale are running “Snack Recovery Missions” around Mickey’s Clubhouse after Pluto keeps “misplacing” everyone’s treats.\nFor each mission, they record:\n\nminutes_to_recover (response): How long it took to recover the snack stash (minutes)\ndistance_m: Total distance traveled during the mission (meters)\nsteps: Total steps taken (wearable tracker)\ncalories: Calories burned during the mission\nwrong_turns: Number of wrong turns / false leads\nrainbow_map: Whether they used Minnie’s Rainbow Map (0/1), which helps reduce time."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-5",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-5",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nFitting the full model,\n\n\nm_full &lt;- glm(minutes_to_recover ~ distance_m + steps + calories + wrong_turns + rainbow_map,\n              family = \"gaussian\",\n              data = chip_dale)\ntidy(m_full)"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-6",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-6",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nFor completeness, let’s check the assumptions,\n\n\nm_full %&gt;% reg_check()"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-7",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-7",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nLet’s also check for outliers,\n\n\nchip_dale &lt;- chip_dale %&gt;%\n  mutate(outlier =  if_else(abs(rstandard(m_full))&gt;2.5, \"Suspected\", \"Not Suspected\"))\nchip_dale %&gt;% count(outlier)"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-8",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-8",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nLet’s check Cook’s distance,\n\n\nm_full %&gt;% cooks()"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-9",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-9",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nFinally, let’s look at multicollinearity,\n\n\nm_full %&gt;% car::vif()\n\n distance_m       steps    calories wrong_turns rainbow_map \n  19.567651   16.198916   14.287939    1.005378    1.007974 \n\n\n\nYIKES! Distance, steps, and calories indicate multicollinearity.\n\nScientifically, this makes sense: they are all measuring “effort,” but in different ways."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-10",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-10",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nHow do we handle multicollinearity?\n\nSimple: remove one or more of the correlated predictors from the model.\n\ne.g., in our example, we could remove distance and steps, keeping only calories.\n\nAdvanced: combine the correlated predictors into a single predictor using techniques such as principal component analysis (PCA) or factor analysis.\n\ne.g., in our example, we could create an “effort” score that combines distance, steps, and calories.\n\n\nWhen this happens, I will figure out specifically what variables do not play well with others.\n\nThen, I will have a conversation with the person I’m working with to determine which variable(s) make the most sense to keep in the model.\n\ne.g., in our example, maybe we are focused on calories burned, so we keep that variable rather than the distance walked or the steps taken."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-11",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-11",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nLet’s fit a reduced model without distance and steps,\n\n\nm_reduced &lt;- glm(minutes_to_recover ~ calories + wrong_turns + rainbow_map,\n              family = \"gaussian\",\n              data = chip_dale)\ntidy(m_reduced)"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-12",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-12",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nChecking multicollinearity,\n\n\nm_reduced %&gt;% car::vif()\n\n   calories wrong_turns rainbow_map \n   1.004000    1.002762    1.001916"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-13",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-13",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nWhat about other diagnostics?"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-14",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-14",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nWhat about other diagnostics?\n\n\nchip_dale %&gt;% \n  mutate(outlier =  if_else(abs(rstandard(m_full))&gt;2.5, \"Suspected\", \"Not Suspected\")) %&gt;% \n  count(outlier)\n\n\n  \n\n\nchip_dale %&gt;% \n  mutate(outlier =  if_else(abs(rstandard(m_reduced))&gt;2.5, \"Suspected\", \"Not Suspected\")) %&gt;% \n  count(outlier)"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-15",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-multicollinearity-15",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Multicollinearity",
    "text": "Model Diagnostics: Multicollinearity\n\nWhat about other diagnostics?"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Sensitivity Analysis",
    "text": "Model Diagnostics: Sensitivity Analysis\n\nThe concept of sensitivity analysis has been mentioned on previous slides.\nThis is where we remove specific “problem points” (outliers or influential points) from the dataset and refit the model to see how much the results change.\nLet’s consider the final model from the Chip and Dale example,\n\n\nm_reduced %&gt;% tidy(conf.int = TRUE)\n\n\n  \n\n\n\n\n\\hat{\\text{recover}} = 16.95 + 0.18 \\text{ calories} + 1.92 \\text{ wrong turns} - 3.48 \\text{ rainbow map}"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-1",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-1",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Sensitivity Analysis",
    "text": "Model Diagnostics: Sensitivity Analysis\n\nWhen we checked the potential outliers, we saw\n\n\nchip_dale &lt;- chip_dale %&gt;% \n  mutate(outlier =  if_else(abs(rstandard(m_reduced))&gt;2.5, \"Suspected\", \"Not Suspected\")) \nchip_dale %&gt;% count(outlier) \n\n\n  \n\n\n\n\nThere are 2 potential outliers."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-2",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-2",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Sensitivity Analysis",
    "text": "Model Diagnostics: Sensitivity Analysis\n\nLet’s check these observations:\n\n\nchip_dale %&gt;% filter(outlier == \"Suspected\") %&gt;% head()\n\n\n  \n\n\n\n\nLooking at summary statistics,\n\n\nchip_dale %&gt;% mean_median(minutes_to_recover, distance_m, steps, calories, wrong_turns) \n\n# A tibble: 5 × 3\n  variable           mean_sd        median_iqr    \n  &lt;chr&gt;              &lt;chr&gt;          &lt;chr&gt;         \n1 calories           70.2 (17.7)    70.4 (23.3)   \n2 distance_m         1200.5 (338.6) 1196.0 (419.4)\n3 minutes_to_recover 22.0 (5.1)     21.9 (6.8)    \n4 steps              1606.7 (505.3) 1615.8 (691.5)\n5 wrong_turns        3.0 (1.6)      3.0 (2.0)"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-3",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-3",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Sensitivity Analysis",
    "text": "Model Diagnostics: Sensitivity Analysis\n\nLet’s also revisit Cook’s distance,"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-4",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-4",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Sensitivity Analysis",
    "text": "Model Diagnostics: Sensitivity Analysis\n\nIn our diagnostics for extreme values, we have found that observation 29 is both an issue as an outlier and as a leverage point, while observation 40 only shows up as an outlier.\nLet’s consider the following modeling situations:\n\nFull dataset – do not remove either observation.\nRemove observation 29 only.\nRemove observation 29 and 40.\n\nWe will then compare the results across these three situations."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-5",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-5",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Sensitivity Analysis",
    "text": "Model Diagnostics: Sensitivity Analysis\n\nThere are multiple ways to approach this in R.\n\nWe could create separate datasets removing values.\nWe could use tidyverse functions to filter within the modeling function.\n\nFor simplicity, we will use three datasets:\n\nchip_dale: original (full) dataset.\nchip_dale_29: dataset without observation 29.\nchip_dale_29_40: dataset without observations 29 and 40.\n\nThe data steps are as follows,\n\n\nchip_dale_29 &lt;- chip_dale %&gt;% filter(mission_id != 29)\nchip_dale_29_40 &lt;- chip_dale %&gt;% filter(!mission_id %in% c(29,40))"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-6",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-6",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Sensitivity Analysis",
    "text": "Model Diagnostics: Sensitivity Analysis\n\nNow, we fit the three models,\n\n\nm_full &lt;- glm(minutes_to_recover ~ calories + wrong_turns + rainbow_map,\n              family = \"gaussian\",\n              data = chip_dale)\nm_29 &lt;- glm(minutes_to_recover ~ calories + wrong_turns + rainbow_map,\n              family = \"gaussian\",\n              data = chip_dale_29)\nm_29_40 &lt;- glm(minutes_to_recover ~ calories + wrong_turns + rainbow_map,\n              family = \"gaussian\",\n              data = chip_dale_29_40)"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-7",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-7",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Sensitivity Analysis",
    "text": "Model Diagnostics: Sensitivity Analysis\n\nPutting the results together in a table,\n\n\n\n\nTerm\nFull\nNo 29\nNo 29 or 40\n\n\n\n\n(Intercept)\n7.10 (p &lt; 0.001)\n6.90 (p &lt; 0.001)\n7.15 (p &lt; 0.001)\n\n\ncalories\n0.16 (p &lt; 0.001)\n0.16 (p &lt; 0.001)\n0.16 (p &lt; 0.001)\n\n\nwrong turns\n1.75 (p &lt; 0.001)\n1.81 (p &lt; 0.001)\n1.79 (p &lt; 0.001)\n\n\nrainbow map\n-3.88 (p &lt; 0.001)\n-3.20 (p &lt; 0.001)\n-3.29 (p &lt; 0.001)\n\n\n\n\nQuestions to ask:\n\nDo the coefficient estimates change meaningfully? Is the scientific interpretation the “same”?\nDo the statistical significances change?"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-8",
    "href": "files/lectures/W1_L4_diagnostics.html#model-diagnostics-sensitivity-analysis-8",
    "title": "Model Diagnostics",
    "section": "Model Diagnostics: Sensitivity Analysis",
    "text": "Model Diagnostics: Sensitivity Analysis\n\nRemoving these observations did not meaningfully change the coefficient estimates or their statistical significance.\n\nAcross models, the scientific interpretations remained approximately the same.\n\nDirection did not change.\nValue of coefficients is similar across models.\n\nAll predictors remained statistically significant at \\alpha = 0.05, specifically with p &lt; 0.001.\n\nSo what would I do?\n\nMy itention would be to keep all observations.\nI would bring the results of the sensitivity analysis up when discussing the results with collaborators, and ask if they see anything “weird” about those specific observations."
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#wrap-up",
    "href": "files/lectures/W1_L4_diagnostics.html#wrap-up",
    "title": "Model Diagnostics",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nThis lecture has covered the basics of model diagnostics related to model fit, outliers, leverage/influence, and multicollinearity.\nWe also discussed sensitivity analysis as a way to assess the impact of “problem points”.\nWe must have a nonstatistical justification for removing any data points from the final analysis.\n\nAlways document any decisions made regarding inclusion or exclusion of data points.\n\nWe are helping others explore their data on the quantitative side, but we need their domain expertise to make final decisions.\nNext module: incorporating categorical predictors"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#app-m-full",
    "href": "files/lectures/W1_L4_diagnostics.html#app-m-full",
    "title": "Model Diagnostics",
    "section": "Appendix: m_full results",
    "text": "Appendix: m_full results\n\nFor the full model,\n\n\nm_full &lt;- glm(minutes_to_recover ~ calories + wrong_turns + rainbow_map,\n              family = \"gaussian\",\n              data = chip_dale)\nm_full %&gt;% tidy(conf.int = TRUE) %&gt;% select(term, estimate, conf.low, conf.high, p.value)"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#app-m-29",
    "href": "files/lectures/W1_L4_diagnostics.html#app-m-29",
    "title": "Model Diagnostics",
    "section": "Appendix: m_29 results",
    "text": "Appendix: m_29 results\n\nFor the model without observation 29,\n\n\nm_29 &lt;- glm(minutes_to_recover ~ calories + wrong_turns + rainbow_map,\n              family = \"gaussian\",\n              data = chip_dale_29)\nm_29 %&gt;% tidy(conf.int = TRUE) %&gt;% select(term, estimate, conf.low, conf.high, p.value)"
  },
  {
    "objectID": "files/lectures/W1_L4_diagnostics.html#app-m-29-40",
    "href": "files/lectures/W1_L4_diagnostics.html#app-m-29-40",
    "title": "Model Diagnostics",
    "section": "Appendix: m_29_40 results",
    "text": "Appendix: m_29_40 results\n\nFor the model without observations 29 and 40,\n\n\nm_29_40 &lt;- glm(minutes_to_recover ~ calories + wrong_turns + rainbow_map,\n              family = \"gaussian\",\n              data = chip_dale_29_40)\nm_29_40 %&gt;% tidy(conf.int = TRUE) %&gt;% select(term, estimate, conf.low, conf.high, p.value)"
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#introduction",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#introduction",
    "title": "Categorical Predictors: Modeling",
    "section": "Introduction",
    "text": "Introduction\n\nRecall the general linear model,  y = \\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k + \\varepsilon \nIn the last lecture, we introduced the concept of categorical predictors.\n\nRemember, if a categorical predictor has c classes, we include c-1 in the model.\n\nWe also discussed how to deal with categorical variables either as a factor variable or as a set of indicator variables."
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#lecture-example-set-up",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#lecture-example-set-up",
    "title": "Categorical Predictors: Modeling",
    "section": "Lecture Example Set Up",
    "text": "Lecture Example Set Up\n\n\nRecall our example dataset of duck-related incidents,\n\n\nduck_incidents &lt;- read_csv(\"https://raw.githubusercontent.com/samanthaseals/SDSII/refs/heads/main/files/data/lectures/W2_duck_incidents.csv\") %&gt;%\n  mutate(loc_yard = if_else(location == \"Backyard\", 1, 0),\n         loc_garage = if_else(location == \"Garage\", 1, 0),\n         loc_kitchen = if_else(location == \"Kitchen\", 1, 0),\n         loc_living = if_else(location == \"Living Room\", 1, 0))\nduck_incidents %&gt;% head()"
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#modeling-with-categorical-predictors",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#modeling-with-categorical-predictors",
    "title": "Categorical Predictors: Modeling",
    "section": "Modeling with Categorical Predictors",
    "text": "Modeling with Categorical Predictors\n\nRecall that for a categorical variable with c categories, we include c-1 indicator variables in the model.\n\nIf we specify a factor variable in our model, R takes care of the indicator variable creation for us.\n\nThe category that is not included in the model is called the reference group.\n\nWe will not be discussing interpretations in this lecture, so this detail is just for context.\nChoice of reference group is important for interpretation, but is arbitrary for model fit and predictions."
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#modeling-with-categorical-predictors-1",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#modeling-with-categorical-predictors-1",
    "title": "Categorical Predictors: Modeling",
    "section": "Modeling with Categorical Predictors",
    "text": "Modeling with Categorical Predictors\n\nHow does R handle reference groups when modeling?\n\nIf we include a factor variable in the model, R automatically creates the indicator variables and selects the reference group (default is the “first” level).\nIf we include indicator variables in the model, we must ensure that we include only c-1 of them.\n\nThe cth one is the reference group.\nIf we include all c, R will not return an estimate for one of the categorical terms.\n\nIf you are using SAS, it will also skip an estimate for one of the categorical terms, but it will give you a warning about it. The warning shown depends on the PROC in use."
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#modeling-with-categorical-predictors-2",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#modeling-with-categorical-predictors-2",
    "title": "Categorical Predictors: Modeling",
    "section": "Modeling with Categorical Predictors",
    "text": "Modeling with Categorical Predictors\n\nWhy will all programs skip an estimate? Let’s consider the nephew variable in our dataset,\n\n\n\n\nNephew\nx_{\\text{H}}\nx_{\\text{D}}\nx_{\\text{L}}\n\n\n\n\nHuey\n1\n0\n0\n\n\nDewey\n0\n1\n0\n\n\nLouie\n0\n0\n1\n\n\n\n\nWe only need two indicators to represent the three categories.\n\nSuppose Louie is the reference group. That means we include x_{\\text{H}} and x_{\\text{D}} in the model.\nWhen x_{\\text{H}} = 1 and x_{\\text{D}} = 0, we know nephew = \\text{Huey}.\nWhen x_{\\text{H}} = 0 and x_{\\text{D}} = 1, we know nephew = \\text{Dewey}.\nWhen x_{\\text{H}} = 0 and x_{\\text{D}} = 0, we know nephew = \\text{Louie}."
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#modeling-with-categorical-predictors-3",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#modeling-with-categorical-predictors-3",
    "title": "Categorical Predictors: Modeling",
    "section": "Modeling with Categorical Predictors",
    "text": "Modeling with Categorical Predictors\n\nNow, let’s consider this from the mathematical standpoint.\nIf we included all three indicators, we would have the following in our model:\n\n\ny = \\beta_0 + \\beta_1 x_{\\text{H}} + \\beta_2 x_{\\text{D}} + \\beta_3 x_{\\text{L}} + \\varepsilon\n\n\nHowever, we know that x_{\\text{L}} = 1 - x_{\\text{H}} - x_{\\text{D}}. i.e., x_{\\text{L}} is a linear combination of x_{\\text{H}} and x_{\\text{D}}.\n\n\ny = \\beta_0 + \\beta_1 x_{\\text{H}} + \\beta_2 x_{\\text{D}} + \\beta_3 (1 - x_{\\text{H}} - x_{\\text{D}}) + \\varepsilon\n\n\nThe algebraically simplified model is not identifiable because we cannot uniquely estimate all of the \\beta terms.\n\n\ny = (\\beta_0 + \\beta_3) + (\\beta_1 - \\beta_3) x_{\\text{H}} + (\\beta_2 - \\beta_3) x_{\\text{D}} + \\varepsilon"
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#modeling-with-categorical-predictors-4",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#modeling-with-categorical-predictors-4",
    "title": "Categorical Predictors: Modeling",
    "section": "Modeling with Categorical Predictors",
    "text": "Modeling with Categorical Predictors\n\nHow do we change reference groups for modeling/testing purposes?\n\nIf using a factor variable, we can relevel the factor using the factor() function with the levels argument.\nIf using indicator variables, we simply update the set of c-1 indicators in the model.\n\nPrepare yourself to see a lot of “if you are using a factor variable… but if you are using indicator variables…” throughout the course.\n\nThis is because of the different ways that categorical variables can be stored in datasets."
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#modeling-with-categorical-predictors-5",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#modeling-with-categorical-predictors-5",
    "title": "Categorical Predictors: Modeling",
    "section": "Modeling with Categorical Predictors",
    "text": "Modeling with Categorical Predictors\n\nWhat is my approach (in practice) to choosing a reference group?\n\nIf there is some natural order, I select the lowest as the reference.\nIf there is not an order, I will select one that “makes sense” as the comparison group when thinking of the context surrounding the question being asked of the data.\nWhen I am communicating results to collaborators, I will explain my choice and then ask if they would like a different reference group."
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#modeling-with-categorical-predictors-r",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#modeling-with-categorical-predictors-r",
    "title": "Categorical Predictors: Modeling",
    "section": "Modeling with Categorical Predictors (R)",
    "text": "Modeling with Categorical Predictors (R)\n\nTo actually model with categorical predictors in R, we use the appropriate modeling function as usual.\n\nNote that the distribution we assume is what drives the choice of function while the type of predictor affects the syntax within the function.\n\nIf we are using stored-as-factor variables, we simply include them,\n\n\nm &lt;- glm(y ~ continuous_var + continuous_var + factor_var +.....)\n\n\nIf we are using numerically stored categorical predictors, we can wrap them with as.factor(),\n\n\nm &lt;- glm(y ~ continuous_var + continuous_var + as.factor(numeric_var) +.....)\n\n\nIf we are using indicator variables, we include c-1 of them,\n\n\nm &lt;- glm(y ~ continuous_var + continuous_var + indicator_1 + ... + indicator_(c-1) +.....)"
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-1-one-categorical-predictor",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-1-one-categorical-predictor",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 1: One Categorical Predictor",
    "text": "Example 1: One Categorical Predictor\n\nLet’s model damage cost using only the location variable.\n\n\nm1 &lt;- glm(damage_cost ~ location, \n          family = \"gaussian\",\n          data = duck_incidents)\ntidy(m1, conf.int = TRUE)\n\n\n  \n\n\n\n\nHere, R has automatically created the indicator variables for us and selected “Backyard” as the reference group (alphabetically first)."
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-1-one-categorical-predictor-1",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-1-one-categorical-predictor-1",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 1: One Categorical Predictor",
    "text": "Example 1: One Categorical Predictor\n\nLet’s now model damage cost using the related indicator variables instead.\n\n\nm2 &lt;- glm(damage_cost ~ loc_garage + loc_kitchen + loc_living,\n          family = \"gaussian\",\n          data = duck_incidents)\ntidy(m2, conf.int = TRUE)\n\n\n  \n\n\n\n\nThis is the same output as we saw when using the factor variable instead."
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-1-one-categorical-predictor-2",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-1-one-categorical-predictor-2",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 1: One Categorical Predictor",
    "text": "Example 1: One Categorical Predictor\n\nPutting them side-by-side,\n\n\ntidy(m1, conf.int = TRUE)\n\n\n  \n\n\ntidy(m2, conf.int = TRUE)"
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-1-one-categorical-predictor-3",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-1-one-categorical-predictor-3",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 1: One Categorical Predictor",
    "text": "Example 1: One Categorical Predictor\n\nRemember, we will want to restate the model in mathematical form:\n\n\ncoefficients(m1)\n\n        (Intercept)      locationGarage     locationKitchen locationLiving Room \n          202.77983            64.34182           131.84687            58.69676 \n\n\n\n\\hat{\\text{cost}} = 202.78 + 64.34 \\text{ garage} + 131.85 \\text{ kitchen} + 58.70 \\text{ living}\n\n\nFrom here, we can easily create the estimated costs for each location:\n\nBackyard (reference): $202.78\nGarage: $202.78 + $64.34 = $267.12\nKitchen: $202.78 + $131.85 = $334.63\nLiving Room: $202.78 + $58.70 = $261.48"
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#intepreting-categorical-predictors",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#intepreting-categorical-predictors",
    "title": "Categorical Predictors: Modeling",
    "section": "Intepreting Categorical Predictors",
    "text": "Intepreting Categorical Predictors\n\nWe know that for continuous predictors, the interpretation is “for a one unit increase in x_i, we expect the outcome to [increase or decrease] by \\beta_i units, holding all other predictors constant.”\nFor categorical predictors, what is a one unit increase in x_j?\n\nFor an indicator variable, a one unit increase means going from not being in that category to being in that category.\nFor a factor variable, a one unit increase means going from the reference group to the category represented by that term."
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#intepreting-categorical-predictors-1",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#intepreting-categorical-predictors-1",
    "title": "Categorical Predictors: Modeling",
    "section": "Intepreting Categorical Predictors",
    "text": "Intepreting Categorical Predictors\n\nThus, the interpretation for categorical predictors becomes “for those in category j, as compared to the reference group, we expect the outcome to [increase or decrease] by \\beta_j units, holding all other predictors constant.”\n\nThat is, \\beta_j represents the difference in the expected outcome between those in category j and those in the reference group."
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-1-intepretations",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-1-intepretations",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 1: Intepretations",
    "text": "Example 1: Intepretations\n\nUsing our previous model output,\n\n\ncoefficients(m1)\n\n        (Intercept)      locationGarage     locationKitchen locationLiving Room \n          202.77983            64.34182           131.84687            58.69676 \n\n\n\nWe can interpret the model estimates as follows:\n\nThe average cost of damage for incidents that occurred in the Backyard is $202.78.\nFor incidents that occurred in the Garage, we expect the damage cost to increase by $64.34.\nFor incidents that occurred in the Kitchen, we expect the damage cost to increase by $131.85.\nFor incidents that occurred in the Living Room, we expect the damage cost to increase $58.70."
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-1-changing-reference-groups",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-1-changing-reference-groups",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 1: Changing Reference Groups",
    "text": "Example 1: Changing Reference Groups\n\nWe have said that the choice of reference group is arbitrary mathematically. Let’s prove this to ourselves.\nLet’s now model damage cost using location but with “Kitchen” as the reference group.\n\n\nm3 &lt;- glm(damage_cost ~ loc_yard + loc_garage + loc_living,\n          family = \"gaussian\",\n          data = duck_incidents)\ntidy(m3, conf.int = TRUE)\n\n\n  \n\n\n\n\nWe see that the estimates themselves have changed.\n\nIn future lectures, we will see that model fit and diagnostics remain the same."
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-1-changing-reference-groups-1",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-1-changing-reference-groups-1",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 1: Changing Reference Groups",
    "text": "Example 1: Changing Reference Groups\n\nLet’s compare the estimates from the two models side-by-side.\n\n\ncoefficients(m2)\n\n(Intercept)  loc_garage loc_kitchen  loc_living \n  202.77983    64.34182   131.84687    58.69676 \n\ncoefficients(m3)\n\n(Intercept)    loc_yard  loc_garage  loc_living \n  334.62670  -131.84687   -67.50505   -73.15011 \n\n\n\nThe intercepts tell us the expected damage cost for the reference groups.\n\nThe average cost for the backyard is $202.78.\nThe average cost for the kitchen is $334.63.\n\nRemember that the coefficient for loc_kitchen was $131.85 – this is the difference between the backyard and the kitchen averages.\n\n$334.63 - $202.78 = $131.85"
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-2-two-categorical-predictors",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-2-two-categorical-predictors",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 2: Two Categorical Predictors",
    "text": "Example 2: Two Categorical Predictors\n\nLet’s now model damage cost using both the location and nephew variables.\n\n\nm4 &lt;- glm(damage_cost ~ location + nephew,\n          family = \"gaussian\",\n          data = duck_incidents)\ntidy(m4, conf.int = TRUE)"
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-2-two-categorical-predictors-1",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-2-two-categorical-predictors-1",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 2: Two Categorical Predictors",
    "text": "Example 2: Two Categorical Predictors\n\nRestating our model mathematically,\n\n\ncoefficients(m4)\n\n        (Intercept)      locationGarage     locationKitchen locationLiving Room \n         139.288148           51.491118          113.783352           66.873723 \n         nephewHuey         nephewLouie \n          -7.974478          206.606636 \n\n\n\n\\begin{align*}\n\\hat{\\text{cost}} = 139.29 &+ 51.49 \\text{ garage} + 113.78 \\text{ kitchen} + 66.87 \\text{ living} \\\\\n& - 7.97 \\text{ Huey} + 206.61 \\text{ Louie}\n\\end{align*}\n\n\nWe now have two reference groups!\n\nLocation: Backyard\nNephew: Dewey"
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-2-two-categorical-predictors-2",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-2-two-categorical-predictors-2",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 2: Two Categorical Predictors",
    "text": "Example 2: Two Categorical Predictors\n\n\\begin{align*}\n\\hat{\\text{cost}} = 139.29 &+ 51.49 \\text{ garage} + 113.78 \\text{ kitchen} + 66.87 \\text{ living} \\\\\n& - 7.97 \\text{ Huey} + 206.61 \\text{ Louie}\n\\end{align*}\n\n\nThe intercept represents the expected damage cost for incidents that occurred in the backyard with Dewey present.\nLocations:\n\nThe garage increases expected damage cost by $51.49.\nThe kitchen increases expected damage cost by $113.78.\nThe living room increases expected damage cost by $66.87.\n\nNephews:\n\nHaving Huey present decreases expected damage cost by $7.97.\nHaving Louie present increases expected damage cost by $206.61."
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-3-mixing-continuous-and-categorical-predictors",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-3-mixing-continuous-and-categorical-predictors",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nLet’s now model damage cost using location, nephew, and sugar_grams.\n\n\nm5 &lt;- glm(damage_cost ~ location + nephew + sugar_grams,\n          family = \"gaussian\",\n          data = duck_incidents)\ntidy(m5, conf.int = TRUE)"
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-3-mixing-continuous-and-categorical-predictors-1",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-3-mixing-continuous-and-categorical-predictors-1",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\nRestating our model mathematically,\n\n\ncoefficients(m5)\n\n        (Intercept)      locationGarage     locationKitchen locationLiving Room \n          62.162513           54.773465           92.647154           58.373882 \n         nephewHuey         nephewLouie         sugar_grams \n          -0.899301          198.792202            1.835897 \n\n\n\n\\begin{align*}\n\\hat{\\text{cost}} = 62.16 &+ 54.77 \\text{ garage} + 92.65 \\text{ kitchen} + 58.37 \\text{ living} \\\\\n& - 0.90 \\text{ Huey} + 198.79 \\text{ Louie} \\\\\n& + 1.84 \\text{ sugar}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-3-mixing-continuous-and-categorical-predictors-2",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-3-mixing-continuous-and-categorical-predictors-2",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 3: Mixing Continuous and Categorical Predictors",
    "text": "Example 3: Mixing Continuous and Categorical Predictors\n\n\\begin{align*}\n\\hat{\\text{cost}} = 62.16 &+ 54.77 \\text{ garage} + 92.65 \\text{ kitchen} + 58.37 \\text{ living} \\\\\n& - 0.90 \\text{ Huey} + 198.79 \\text{ Louie} \\\\\n& + 1.84 \\text{ sugar}\n\\end{align*}\n\n\nThe interpretation of predictors remains the same as we have previously learned.\n\nThe garage costs an additional $54.77\nThe kitchen costs an additional $92.65\nThe living room costs an additional $58.37\nHaving Huey present decreases expected damage cost by $0.90\nHaving Louie present increases expected damage cost by $198.79\nFor each additional gram of sugar intake, we expect the damage cost to increase by $1.84"
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-4-binary-predictors",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-4-binary-predictors",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 4: Binary Predictors",
    "text": "Example 4: Binary Predictors\n\nRecall the punished binary indicator we created that identifies if Donald punishes the nephews (either “Assigns Chores” or “Grounds”) or not (either “Laughs” or “Yells”).\n\n\nduck_incidents &lt;- duck_incidents %&gt;%\n  mutate(donald_action = case_when(donald_reaction %in% c(\"Assigns Chores\", \"Grounds\") ~ \"Punished\",\n                                   donald_reaction %in% c(\"Laughs\", \"Yells\") ~ \"No Punishment\")) %&gt;%\n  mutate(punished = if_else(donald_action == \"Punished\", 1, 0))\n\n\nDouble checking,\n\n\nkable(duck_incidents %&gt;% n_pct(donald_reaction, punished))\n\n\n\n\ndonald_reaction\n0\n1\n\n\n\n\nAssigns Chores\n0 (0.0%)\n83 (39.3%)\n\n\nGrounds\n0 (0.0%)\n128 (60.7%)\n\n\nLaughs\n70 (29.3%)\n0 (0.0%)\n\n\nYells\n169 (70.7%)\n0 (0.0%)"
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-4-binary-predictors-1",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-4-binary-predictors-1",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 4: Binary Predictors",
    "text": "Example 4: Binary Predictors\n\nLet’s now model damage cost using sugar_grams and punished.\n\n\nm6 &lt;- glm(damage_cost ~ sugar_grams + punished,\n          family = \"gaussian\",\n          data = duck_incidents)\ntidy(m6, conf.int = TRUE)\n\n\n  \n\n\n\n\nComparing this to the model using the donald_action factor variable instead,\n\n\nm7 &lt;- glm(damage_cost ~ sugar_grams + donald_action,\n          family = \"gaussian\",\n          data = duck_incidents)\ntidy(m7, conf.int = TRUE)"
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-4-binary-predictors-2",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#example-4-binary-predictors-2",
    "title": "Categorical Predictors: Modeling",
    "section": "Example 4: Binary Predictors",
    "text": "Example 4: Binary Predictors\n\nMathematically, our model is\n\n\n\\hat{\\text{cost}} =  86.61 + 3.80 \\text{ sugar} + 8.28 \\text{ punished}\n\n\nInterpreting our model coefficients,\n\nThe average cost of damage when Donald does not punish the nephews and no sugar is involved is $86.61.\nFor each additional gram of sugar intake, we expect the damage cost to increase by $3.80.\nWhen Donald punishes the nephews, we expect the damage cost to increase by $8.28."
  },
  {
    "objectID": "files/lectures/W2_L2_cat_pred_mod_interp.html#wrap-up",
    "href": "files/lectures/W2_L2_cat_pred_mod_interp.html#wrap-up",
    "title": "Categorical Predictors: Modeling",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nIn this lecture, we have introduced modeling with categorical predictors.\nAlways remember that if there are c categories, there will be c-1 predictor terms in the model.\nWhether we use a true factor variable or we use a set of indicator variables, the model fit and predictions will be the same.\nThe choice of reference group is arbitrary mathematically, but important for interpretation.\nIn the next lecture, we will discuss statistical inference with categorical predictors."
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#introduction",
    "href": "files/lectures/W1_L3_assumptions.html#introduction",
    "title": "Model Assumptions",
    "section": "Introduction",
    "text": "Introduction\n\nRecall the glm,\n\n\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... \\beta_k x_k + \\varepsilon\n\n\nIn this model, \\varepsilon is the residual error term.\nRecall that the residual error (how far away the observation is from the response surface) is defined as\n\n\\varepsilon = y - \\hat{y}\n\nNote that\n\ny is the observed value\n\\hat{y} is the predicted value"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-hileg",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-hileg",
    "title": "Model Assumptions",
    "section": "Model Assumptions: HILE+G",
    "text": "Model Assumptions: HILE+G\n\nTo ensure the validity of the least squares estimation, we have several assumptions:\n\nHomogeneity of variance\nIndependence of observations\nLinearity\nExistence\nGaussian errors\n\nThink: HILE+G\n\nWe require HILE for the OLS to be valid.\nWe require +G for maximum likelihood estimation."
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Homogeneity of Variance",
    "text": "Model Assumptions: Homogeneity of Variance\n\nHomogeneity of variance (also called homoscedasticity) means that the variance of the residuals is constant across all levels of the predictors.\nIf the variance of the residuals changes at different levels of the predictors, we have heteroscedasticity.\nWe will check this assumption graphically using a scatterplot of residual vs. fitted values.\n\nWe do not want to see a pattern.\nA “funnel” or “open fan” shape indicates heteroscedasticity.\nA general “cloud” shape indicates homoscedasticity."
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance-r",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance-r",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Homogeneity of Variance (R)",
    "text": "Model Assumptions: Homogeneity of Variance (R)\n\nWe will use the ssstats package to check assumptions.\nFor variance, we can use the variance_check() function.\n\n\nmodel %&gt;% variance_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance-1",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance-1",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Homogeneity of Variance",
    "text": "Model Assumptions: Homogeneity of Variance\n\nSuppose we have a model, m1,\n\n\n\\hat{y} = 3 + 1.5 x + \\hat{\\varepsilon}\n\n\nm1 %&gt;% variance_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance-2",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance-2",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Homogeneity of Variance",
    "text": "Model Assumptions: Homogeneity of Variance\n\nSuppose we have a model, m2,\n\n\n\\hat{y} = 4 + 1.2 x_1 - 0.8 x_2 + \\hat{\\varepsilon}\n\n\nm2 %&gt;% variance_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance-3",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance-3",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Homogeneity of Variance",
    "text": "Model Assumptions: Homogeneity of Variance\n\nSuppose we have a model, m3,\n\n\n\\hat{y} = 5 + 1.1 x_1 - 0.9 x_2 + \\hat{\\varepsilon}\n\n\nm3 %&gt;% variance_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance-4",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance-4",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Homogeneity of Variance",
    "text": "Model Assumptions: Homogeneity of Variance\n\nSuppose we have a model, m4,\n\n\n\\hat{y} = 4 + 1.0 x_1 - 0.7 x_2 + \\hat{\\varepsilon}\n\n\nm4 %&gt;% variance_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance-5",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance-5",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Homogeneity of Variance",
    "text": "Model Assumptions: Homogeneity of Variance\n\nSuppose we have a model, m5,\n\n\n\\hat{y} = 3 + 1.3 x_1 - 0.9 x_2 + \\hat{\\varepsilon}\n\n\nm5 %&gt;% variance_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance-6",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-homogeneity-of-variance-6",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Homogeneity of Variance",
    "text": "Model Assumptions: Homogeneity of Variance\n\nSuppose we have a model, m6,\n\n\n\\hat{y} = 2 + 1.1 x_1 - 0.8 x_2 + \\hat{\\varepsilon}\n\n\nm6 %&gt;% variance_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-independence-of-observations",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-independence-of-observations",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Independence of Observations",
    "text": "Model Assumptions: Independence of Observations\n\nWe also assume that our observations are independent.\n\nThis means that the value of one observation does not influence or provide information about another observation.\n\nExamples of independent data:\n\nRandomly sampled individuals from a large population.\nMeasurements taken from different individuals with no inherent relationship\n\nExamples of dependent data:\n\nMeasurements taken from the same individual over time (repeated measures)\nMeasurements taken from individuals within the same group or cluster (e.g., students within the same classroom; people in the same family)\nSpatial data where observations are collected from nearby locations."
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-independence-of-observations-1",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-independence-of-observations-1",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Independence of Observations",
    "text": "Model Assumptions: Independence of Observations\n\nViolations of independence often occur in longitudinal, time series, and spatial data.\nWhen analyzing correlated data, the methodology learned in this course is not appropriate.\n\nHonestly, it’s really not a big deal: we include a covariance structure in the model to account for dependence.\nHowever, this methodology is beyond the scope of this course – we will focus only on independent data.\n\nWhat about squicky cases?\n\nWe know that perfect independence may not be achievable in practice.\nMild violations of independence may not severely impact results, but this is context-dependent.\nUltimately, we do the best with what we can and notate any potential limitations in our analysis."
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-linearity",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-linearity",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Linearity",
    "text": "Model Assumptions: Linearity\n\nWe assume that the relationship between each predictor and the response is linear in the parameters (i.e., \\beta_i).\n\nThis means that the effect of a one-unit change in a predictor on the response is constant, regardless of the value of that predictor.\n\nIn this course, all of the models we will construct are linear models.\nThe language gets tricky because we can have non-linear relationships in a linear model.\n\nFor example, we can include polynomial terms (e.g., x^2, x^3) or interaction terms (e.g., x_1 \\times x_2) in a linear model.\nAs long as the model is linear in the parameters, it is considered a linear model."
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-linearity-1",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-linearity-1",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Linearity",
    "text": "Model Assumptions: Linearity\n\nConsider the following models – which are linear in \\beta?\n\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\hat{\\varepsilon}\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\hat{\\varepsilon}\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 x_3 + \\hat{\\varepsilon}\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2^2 x_2 + \\hat{\\varepsilon}\n\\hat{y} = \\beta_0 + \\beta_1 \\log(x_1) + \\beta_2 x_2 + \\hat{\\varepsilon}\n\\hat{y} = \\beta_0 + \\log(\\beta_1) x_1 + \\beta_2 x_2 + \\hat{\\varepsilon}"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-linearity-2",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-linearity-2",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Linearity",
    "text": "Model Assumptions: Linearity\n\nConsider the following models – which are linear in \\beta?\n\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\hat{\\varepsilon} – linear\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\hat{\\varepsilon} – linear\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 x_3 + \\hat{\\varepsilon} – linear\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2^2 x_2 + \\hat{\\varepsilon} – broken by \\beta_2^2\n\\hat{y} = \\beta_0 + \\beta_1 \\log(x_1) + \\beta_2 x_2 + \\hat{\\varepsilon} – linear\n\\hat{y} = \\beta_0 + \\log(\\beta_1) x_1 + \\beta_2 x_2 + \\hat{\\varepsilon} – broken by \\log(\\beta_1)"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-existence",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-existence",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Existence",
    "text": "Model Assumptions: Existence\n\nThe residuals have a finite mean and variance.\n\nMean (first moment): E[X] = \\mu\nVariance (second central moment): E[(X - \\mu)^2] = \\sigma^2\n\nThis assumption is almost always satisfied in practical applications.\n\nWe can think of this as: the residuals are not so extreme that their average or variability is infinite.\n\nThis is not an assumption we check – we will see estimation errors when this assumption is violated."
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Gaussian Distribution",
    "text": "Model Assumptions: Gaussian Distribution\n\nNote! The normal distribution is also known as the Gaussian distribution.\nFor our inference to be valid, we must assume that the residuals are normally distributed.\n\nOtherwise, our estimation may not be accurate (i.e., inference may lead us to the “wrong” decision).\n\nWe will check this assumption using a quantile-quantile (q-q) plot of the residuals.\n\nWe want to see points roughly along the line.\nLarge deviations from the line indicate non-normality.\n\nWe can also use a histogram to “back up” our decision from the q-q plot.\n\nWe want to see it roughly mound-shaped and symmetric.\nSkewed or multi-modal histograms indicate non-normality."
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution-r",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution-r",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Gaussian Distribution (R)",
    "text": "Model Assumptions: Gaussian Distribution (R)\n\nWe will use the ssstats package to check assumptions.\nFor normality, we can use the normality_check() function.\n\n\nmodel %&gt;% normality_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution-1",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution-1",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Gaussian Distribution",
    "text": "Model Assumptions: Gaussian Distribution\n\nRecall our example model, m1:\n\n\n\\hat{y} = 3 + 1.5 x + \\hat{\\varepsilon}\n\n\nm1 %&gt;% normality_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution-2",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution-2",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Gaussian Distribution",
    "text": "Model Assumptions: Gaussian Distribution\n\nRecall our example model, m2:\n\n\n\\hat{y} = 4 + 1.2 x_1 - 0.8 x_2 + \\hat{\\varepsilon}\n\n\nm2 %&gt;% normality_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution-3",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution-3",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Gaussian Distribution",
    "text": "Model Assumptions: Gaussian Distribution\n\nRecall our example model, m3:\n\n\n\\hat{y} = 5 + 1.1 x_1 - 0.9 x_2 + \\hat{\\varepsilon}\n\n\nm3 %&gt;% normality_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution-4",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution-4",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Gaussian Distribution",
    "text": "Model Assumptions: Gaussian Distribution\n\nRecall our example model, m4:\n\n\n\\hat{y} = 4 + 1.0 x_1 - 0.7 x_2 + \\hat{\\varepsilon}\n\n\nm4 %&gt;% normality_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution-5",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution-5",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Gaussian Distribution",
    "text": "Model Assumptions: Gaussian Distribution\n\nRecall our example model, m5:\n\n\n\\hat{y} = 3 + 1.3 x_1 - 0.9 x_2 + \\hat{\\varepsilon}\n\n\nm5 %&gt;% normality_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution-6",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-gaussian-distribution-6",
    "title": "Model Assumptions",
    "section": "Model Assumptions: Gaussian Distribution",
    "text": "Model Assumptions: Gaussian Distribution\n\nRecall our example model, m6:\n\n\n\\hat{y} = 2 + 1.1 x_1 - 0.8 x_2 + \\hat{\\varepsilon}\n\n\nm6 %&gt;% normality_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-r",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-r",
    "title": "Model Assumptions",
    "section": "Model Assumptions (R)",
    "text": "Model Assumptions (R)\n\nTo summarize, our model assumptions can be written as:\n\n\\varepsilon \\overset{\\text{iid}}{\\sim} N(0, \\sigma^2)\n\nThis notation combines our checkable assumptions:\n\nResiduals are independent and identically distributed (I)\nResiduals follow a normal distribution (G), centered at 0 and with some constant variance, \\sigma^2 (H).\n\nWe can look at a graph with all of these checks at once using the reg_check() function in the ssstats package.\n\n\nmodel %&gt;% reg_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions",
    "title": "Model Assumptions",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nConsider this for m1:\n\n\nm1 %&gt;% reg_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-1",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-1",
    "title": "Model Assumptions",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nConsider this for m3:\n\n\nm3 %&gt;% reg_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#model-assumptions-2",
    "href": "files/lectures/W1_L3_assumptions.html#model-assumptions-2",
    "title": "Model Assumptions",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nConsider this for m5:\n\n\nm5 %&gt;% reg_check()"
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#testing-for-normality",
    "href": "files/lectures/W1_L3_assumptions.html#testing-for-normality",
    "title": "Model Assumptions",
    "section": "Testing for Normality…?",
    "text": "Testing for Normality…?\n\nNo.\n\nFormal tests for normality (e.g., Shapiro-Wilk test, Kolmogorov-Smirnov test) are generally not recommended in practice.\nThese tests are sensitive to large sample sizes, leading to the rejection of normality, even for minor infractions that do not affect our inferential conclusions.\n\nAs sample size (n) increases, our standard error (\\text{s.d.} / \\sqrt{n}) decreases, increasing our test statistic (\\text{observed} / \\text{s.e.}_{\\hat{\\beta}_i}), decreasing the p-value…… making it easier to reject H_0: the data follows a normal distribution.\n\nInstead, graphical methods (q-q plots, histograms) and practical considerations are preferred for assessing normality."
  },
  {
    "objectID": "files/lectures/W1_L3_assumptions.html#wrap-up",
    "href": "files/lectures/W1_L3_assumptions.html#wrap-up",
    "title": "Model Assumptions",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nThis lecture has covered the assumptions on the linear model for continuous outcomes assuming independent data and Gaussian errors.\nMy approach in “real life” is:\n\nFit the model.\nCheck assumptions using graphical diagnostics.\nIf assumptions are violated, consider an alternative approach.\nDocument any assumption violations and their potential impact on results.\nDiscuss findings (including relevant failed assumption checks) with collaborators/stakeholders.\n\nNext lecture: Model Diagnostics"
  }
]