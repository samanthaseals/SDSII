---
output: 
  beamer_presentation:
    fig_caption: false
    includes:
      in_header: header.tex
classoption: 
  - "aspectratio=169"
  
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../lectures/PDF") })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.path = "images")

library(tidyverse)
library(gsheet)
library(brant)
library(MASS)
library(fastDummies)
library(lmtest)
library(reshape2)

data <- as_tibble(gsheet2tbl("https://docs.google.com/spreadsheets/d/1Q8ZS-umaaSHbPA3gGEy-UConGON_1_oNR_O6Wpsf7bE/edit?usp=sharing"))
data$Ideology <- as.factor(data$Ideology)

```


## {.standout} 
\vskip12em
\begin{center}{\color{white} \huge \textbf{Ordinal Logistic Regression}} \vskip1em
{\color{white} \Large Statistics for Data Science II}
\end{center}


## Introduction

Suppose our response variable has $c$ ordered categories 
\begin{itemize}
    \item e.g., classification of student: freshman, sophomore, junior, senior
\end{itemize} \vskip.5em

We again will create $c-1$ models.
\begin{itemize}
    \item The $\hat{\beta}_i$ will be the same across the models.
    \item The $\hat{\beta}_0$ will change for each category.
\end{itemize}

## Model

We will use the cumulative logit model,
\begin{align*}
  \text{logit}\left( P[Y \le j] \right) &= \hat{\beta}_{0j} + \hat{\beta}_{1} X_1 + \hdots + \hat{\beta}_{k} X_k \\
  \log \left( \frac{P[Y \le j]}{1 - P[Y \le j]} \right)&= \hat{\beta}_{0j} + \hat{\beta}_{1} X_1 + \hdots + \hat{\beta}_{k} X_k \\
  \log \left( \frac{\pi_1 + \hdots + \pi_j }{\pi_{j+1} + \hdots \pi_{c}} \right) &= \hat{\beta}_{0j} + \hat{\beta}_{1} X_1 + \hdots + \hat{\beta}_{k} X_k
\end{align*}

## Model

As noted previously, the intercept depends on $j$.
\begin{itemize}
    \item This means that curves will have the same shape $\forall \ j$. 
    \item We are just shifting the curve along the $x$-axis, depending on the response category. 
\end{itemize}

This model assumes \textit{proportional odds}.
\begin{itemize}
    \item For each predictor included in the model, the slopes across two outcomes response levels are the same, regardless of which two responses we consider. 
\end{itemize}

## Model

\textbf{Example:}
\begin{itemize}
  \item Consider data from a General Social Survey, relating political ideology to political party affiliation. Political ideology has a five-point ordinal scale, ranging from very liberal ($Y=1$) to very conservative ($Y=5$). Let $X$ be an indicator variable for political party, with $X = 1$ for Democrats and $X = 0$ for Republicans. We will construct an ordinal logistic regression model that models political ideology as a function of political party and sex.
\end{itemize}

## Model

\vskip1em
\textbf{Example:}
```{r}
head(data)
```

## Model

\vskip2em
\textbf{Example:}
\tiny
```{r}
data <- dummy_cols(data, select_columns = c("Party", "Sex"))
m1 <- polr(Ideology ~ Party_Republican + Sex_Male,
           data = data, weights = Count, Hess = TRUE)
summary(m1)
```

## Model

\textbf{Example:}
\begin{itemize}
  \item The resulting models are:
\end{itemize} \vskip-2em
\begin{align*}
  \text{logit}\left( P[Y \le \text{V. Lib.}] \right) &= -1.45 + 0.96 \text{repub.} + 0.12 \text{male} \\
  \text{logit}\left( P[Y \le \text{Lib.}] \right) &= -0.46 + 0.96 \text{repub.} + 0.12 \text{male} \\
  \text{logit}\left( P[Y \le \text{Mod.}] \right) &= 1.26 + 0.96 \text{repub.} +0.12 \text{male} \\
  \text{logit}\left( P[Y \le \text{Cons.}] \right) &= 2.09 + 0.96 \text{repub.} +0.12 \text{male} 
\end{align*}
\begin{itemize}
  \item Note that $P[Y \le \text{V. Cons.}]=1$, thus, does not need a model.
\end{itemize}



## Interpretations

\vskip1em
Odds ratios are interpreted slightly different due to the model being cumulative. 
\begin{itemize}
    \item The change in odds does not depend on the category of the response.
\end{itemize}

For continuous predictors:
\begin{itemize}
  \item For a one [predictor unit] increase in [predictor], the odds in favor of [the response category $j$] or lower, as compared to higher than [the response category $j$], are multiplied by e$^{\hat{\beta}_i}$.
\end{itemize}

For categorical predictors:
\begin{itemize}
  \item As compared to [the reference category], the odds in favor of [the response category $j$] or lower, as compared to higher than [the response category $j$], for [the predictor category of interest] are multiplied by e$^{\hat{\beta}_i}$.
\end{itemize}

## Interpretations

\vskip1em
We can also interpret in terms of a percent increase or decrease

For continuous predictors:
\begin{itemize}
  \item For a one [predictor unit] increase in [predictor], the odds in favor of [the response category $j$] or lower, as compared to higher than [the response category $j$], are [increased or decreased] by [100(e$^{\hat{\beta}_i}$-1)\% or 100(1-e$^{\hat{\beta}_i}$)\%].
\end{itemize}

For categorical predictors:
\begin{itemize}
  \item As compared to [the reference category], the odds in favor of [the response category $j$] or lower, as compared to higher than [the response category $j$], for [the predictor category of interest] are [increased or decreased] by [100(e$^{\hat{\beta}_i}$-1)\% or 100(1-e$^{\hat{\beta}_i}$)\%].
\end{itemize}

## Interpretations

\vskip1em
\textbf{Example:}
\begin{itemize}
  \item For the political ideology data,
\end{itemize}

\small
```{r}
round(exp(coefficients(m1)[1]),2)
```

\normalsize
\begin{itemize}
  \item For any fixed $j$, the estimated odds that a Republican's response is in the conservative direction rather than the liberal direction are $e^{0.9636} = 2.62$ times the estimated odds for Democrats.
\begin{itemize}
  \item This is a 62\% decrease in odds as compared to Democrats.
\end{itemize}
\end{itemize}

## Interpretations

\vskip1em
\textbf{Example:}
\begin{itemize}
  \item For the political ideology data,
\end{itemize}

\small
```{r}
round(exp(coefficients(m1)[2]),2)
```

\normalsize
\begin{itemize}
  \item For any fixed $j$, the estimated odds that a male's response is in the conservative direction rather than the liberal direction are $e^{0.1169} = 1.12$ times the estimated odds for females.
  \begin{itemize}
    \item This is a 12\% increase in odds as compared to females.
  \end{itemize}
\end{itemize}

## Inference

\vskip1em
Like in binary and nominal logistic regressions, we can construct Wald $Z$ statistic using the \texttt{coeftest()} function.

\textbf{Example:}
\tiny
```{r}
coeftest(m1)
```

\normalsize
\begin{itemize}
  \item Party affiliation is a significant predictor of political ideology ($p<0.001$), but biological sex is not ($p=0.359$).
\end{itemize}

## Inference

\vskip2em
Also like in binary and nominal logistic regressions, we can construct confidence intervals by running the model results through the \texttt{confint()} function. \vskip1em

\textbf{Example:}
\scriptsize
```{r}
round(exp(confint(m1)),2)
```

\normalsize
\begin{itemize}
  \item Thus, the 95\% CI for the OR for party affiliation (republican vs. democrat) is (0.30, 0.49) and for biological sex (male vs. female) is (0.88, 1.44).
\end{itemize}


## Predictions

\vskip1em
We can use the resulting models to construct specific predictions. 
\begin{itemize}
  \item This may require some calculations due to the nature of the cumulative models.
  \item In our example, the model order is V. Lib $\to$ Lib. $\to$ Mod. $\to$ Cons. $\to$ V. Cons.
  \begin{itemize}
    \item If we want the individual probabilities,
  \end{itemize}
\end{itemize} \vskip-2em
\begin{align*}
  P[Y = \text{V. Lib}] &= P[Y \le \text{V. Lib}] \\
  P[Y = \text{Lib.}] &= P[Y \le \text{Lib.}] - P[Y \le \text{V. Lib}] \\
  P[Y = \text{Mod.}] &= P[Y \le \text{Mod.}] - P[Y \le \text{Lib.}] \\
  P[Y = \text{Cons.}] &= P[Y \le \text{Cons.}] - P[Y \le \text{Mod.}] \\
  P[Y = \text{V. Cons.}] &= 1 - P[Y \le \text{Cons.}]
\end{align*}

## Predictions

\vskip1em
Using algebra, we can solve the cumulative logit model for $P[Y \le j]$:
\begin{align*}
  \log \left( \frac{P[Y \le j]}{1 - P[Y \le j]} \right)&= \hat{\beta}_{0j} + \hat{\beta}_{1} X_1 + \hdots + \hat{\beta}_{k} X_k \\
  \frac{P[Y \le j]}{1 - P[Y \le j]} &= \exp \left\{ \hat{\beta}_{0j} + \hat{\beta}_{1} X_1 + \hdots + \hat{\beta}_{k} X_k \right\} \\
  & \vdots \\
  P[Y \le j] &= \frac{\exp \left\{ \hat{\beta}_{0j} + \hat{\beta}_{1} X_1 + \hdots + \hat{\beta}_{k} X_k \right\}}{1 + \exp \left\{ \hat{\beta}_{0j} + \hat{\beta}_{1} X_1 + \hdots + \hat{\beta}_{k} X_k \right\}}
\end{align*}

## Predictions

\vskip1em
We can use the \texttt{fitted()} function to find the predicted probabilities of the outcomes.

\textbf{Example}:

\scriptsize
```{r}
head(unique(as.tibble(fitted(m1))))
```


## Visualization

\vskip1em
Ordinal logistic regression models can be visualized like we saw previously with binary and nominal logistic regressions.
\begin{itemize}
  \item Because we do not have a categorical predictor in the example model, we cannot construct the curve as we did before.
\end{itemize}

Let us now consider visualizing the expected probabilities for each group to see if we can see a pattern.
\begin{itemize}
  \item $y$-axis: predicted probability
  \item $x$-axis: possible responses
  \item dots: colored by sex and/or political affiliation
\end{itemize}

## Visualization

\vskip1em
\textbf{Example}:

\scriptsize
```{r}
graph <- data %>% 
  mutate(`Sex and Affiliation` = paste(Sex, Party),
         Ideology = str_remove_all(Ideology, "[12345 -]")) %>%
  mutate(Ideology = ifelse(Ideology == "VeryLiberal", "Very Liberal",
                    ifelse(Ideology == "VeryConservative", "Very Conservative", 
                    Ideology))) %>%
  mutate(`Very Liberal` = as.tibble(fitted(m1))$`1 - Very Liberal`,
         Liberal = as.tibble(fitted(m1))$`2 - Liberal`,
         Moderate = as.tibble(fitted(m1))$`3 - Moderate`,
         Conservative = as.tibble(fitted(m1))$`4 - Conservative`,
         `Very Conservative` = as.tibble(fitted(m1))$`5 - Very Conservative`) %>%
  dplyr::select(`Sex and Affiliation`, `Very Liberal`, Liberal, Moderate,
                Conservative, `Very Conservative`) %>%
  melt(id.vars="Sex and Affiliation", 
       variable.name="Political Ideology",
       value.name="Predicted Probability") %>%
  unique()
```

## Visualization

\textbf{Example}:

\scriptsize
```{r}
plot <- graph %>% ggplot(aes(x = `Political Ideology`, 
                             y = `Predicted Probability`,
                             color = `Sex and Affiliation`)) +
  geom_point(size = 3) + 
  theme_bw()
```

```{r, echo = FALSE}
ggsave("/Volumes/GoogleDrive/My Drive/IAIA/SDSII/lectures/images/w2l3fig1.png")
```

## Visualization

\vskip1em
\textbf{Example:}

```{r, echo = FALSE, fig.align="center", out.width = "60%", fig.alt = "Scatterplot showing predicted probabilities for each political ideology. Political ideology is on the x axis and predicted probability is on the y axis. Markers are colored by sex and political officialation."}
knitr::include_graphics("images/w2l3fig1.png")
```

## Proportional Odds Assumption

As mentioned previously, we are assuming proportional odds. 
\begin{itemize}
  \item This means that the slope is the same, regardless of what response category we're looking at.
\end{itemize} \vskip.5em

We will check this assumption with Brant's test (\href{https://www.jstor.org/stable/2532457}{article here}).
\begin{itemize}
  \item Briefly, this will construct a $\chi^2$ test for every predictor in the model.
  \item If $p<\alpha$, the assumption is broken.
\end{itemize} \vskip.5em

If the assumption is broken, we should step back down to nominal logistic regression.

## Proportional Odds Assumption

\vskip1em
\textbf{Example:} \vskip.5em
\scriptsize
```{r}
brant(m1)
```
\normalsize
\begin{itemize}
  \item All $p > \alpha$, thus, we meet the proportional odds assumption.
\end{itemize}